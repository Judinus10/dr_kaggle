{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18ef2e0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-27T07:01:56.379306Z",
     "iopub.status.busy": "2026-02-27T07:01:56.378650Z",
     "iopub.status.idle": "2026-02-27T07:01:56.876272Z",
     "shell.execute_reply": "2026-02-27T07:01:56.875312Z"
    },
    "papermill": {
     "duration": 0.505195,
     "end_time": "2026-02-27T07:01:56.878284",
     "exception": false,
     "start_time": "2026-02-27T07:01:56.373089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 27 07:01:56 2026       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82621248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:01:56.887060Z",
     "iopub.status.busy": "2026-02-27T07:01:56.886783Z",
     "iopub.status.idle": "2026-02-27T07:01:58.031223Z",
     "shell.execute_reply": "2026-02-27T07:01:58.030186Z"
    },
    "papermill": {
     "duration": 1.150591,
     "end_time": "2026-02-27T07:01:58.032782",
     "exception": false,
     "start_time": "2026-02-27T07:01:56.882191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lesion-aware-dr'...\r\n",
      "remote: Enumerating objects: 201, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (201/201), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (109/109), done.\u001b[K\r\n",
      "remote: Total 201 (delta 94), reused 178 (delta 71), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (201/201), 103.95 KiB | 2.04 MiB/s, done.\r\n",
      "Resolving deltas: 100% (94/94), done.\r\n",
      "/kaggle/working/lesion-aware-dr\n"
     ]
    }
   ],
   "source": [
    " !git clone https://github.com/Judinus10/lesion-aware-dr.git \n",
    "%cd lesion-aware-dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7802e9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:01:58.042641Z",
     "iopub.status.busy": "2026-02-27T07:01:58.042014Z",
     "iopub.status.idle": "2026-02-27T07:02:15.505609Z",
     "shell.execute_reply": "2026-02-27T07:02:15.504870Z"
    },
    "papermill": {
     "duration": 17.470464,
     "end_time": "2026-02-27T07:02:15.507342",
     "exception": false,
     "start_time": "2026-02-27T07:01:58.036878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\r\n",
      "total 3.2M\r\n",
      "drwxr-xr-x 3 nobody nogroup    0 Feb 16 16:26 .\r\n",
      "drwxr-xr-x 3 root   root    4.0K Feb 27 07:01 ..\r\n",
      "-rw-r--r-- 1 nobody nogroup 1.6M Feb 16 16:26 all_labels.csv\r\n",
      "drwxr-xr-x 2 nobody nogroup    0 Feb 16 16:26 Images\r\n",
      "-rw-r--r-- 1 nobody nogroup 1.2M Feb 16 16:26 original_test_labels.csv\r\n",
      "-rw-r--r-- 1 nobody nogroup 455K Feb 16 16:26 original_train_labels.csv\r\n",
      "total 23G\r\n",
      "drwxr-xr-x 2 nobody nogroup    0 Feb 16 16:26 .\r\n",
      "drwxr-xr-x 3 nobody nogroup    0 Feb 16 16:26 ..\r\n",
      "-rw-r--r-- 1 nobody nogroup 218K Feb 16 16:19 10000_left.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 214K Feb 16 16:19 10000_right.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 216K Feb 16 16:19 10001_left.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 211K Feb 16 16:19 10001_right.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 186K Feb 16 16:19 10002_left.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 297K Feb 16 16:19 10002_right.png\r\n",
      "-rw-r--r-- 1 nobody nogroup 210K Feb 16 16:19 10003_left.png\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT=\"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\"\n",
    "\n",
    "!echo $DATA_ROOT\n",
    "!ls -lah $DATA_ROOT\n",
    "!ls -lah $DATA_ROOT/Images | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4de985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:15.517272Z",
     "iopub.status.busy": "2026-02-27T07:02:15.516596Z",
     "iopub.status.idle": "2026-02-27T07:02:15.521560Z",
     "shell.execute_reply": "2026-02-27T07:02:15.520703Z"
    },
    "papermill": {
     "duration": 0.011584,
     "end_time": "2026-02-27T07:02:15.523057",
     "exception": false,
     "start_time": "2026-02-27T07:02:15.511473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from pathlib import Path\n",
      "\n",
      "# --------------------\n",
      "# Paths\n",
      "# --------------------\n",
      "DATA_DIR = Path(r\"E:\\DR_related\\eyepacs\")\n",
      "LABELS_CSV = DATA_DIR / \"all_labels.csv\"\n",
      "IMAGES_DIR = DATA_DIR / \"Images\"\n",
      "\n",
      "OUT_TRAIN = DATA_DIR / \"train.csv\"\n",
      "OUT_VAL = DATA_DIR / \"val.csv\"\n",
      "\n",
      "# --------------------\n",
      "# Load labels\n",
      "# --------------------\n",
      "df = pd.read_csv(LABELS_CSV)\n",
      "print(\"Original columns:\", list(df.columns))\n",
      "\n",
      "# --------------------\n",
      "# Nor\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"/kaggle/working/lesion-aware-dr/scripts/make_split.py\")\n",
    "\n",
    "print(file_path.read_text()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a50de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:15.532479Z",
     "iopub.status.busy": "2026-02-27T07:02:15.532233Z",
     "iopub.status.idle": "2026-02-27T07:02:15.538206Z",
     "shell.execute_reply": "2026-02-27T07:02:15.537515Z"
    },
    "papermill": {
     "duration": 0.012341,
     "end_time": "2026-02-27T07:02:15.539595",
     "exception": false,
     "start_time": "2026-02-27T07:02:15.527254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path updated\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"/kaggle/working/lesion-aware-dr/scripts/make_split.py\")\n",
    "\n",
    "text = file_path.read_text()\n",
    "\n",
    "text = text.replace(\n",
    "    r\"E:\\DR_related\\eyepacs\",\n",
    "    \"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\"\n",
    ")\n",
    "\n",
    "file_path.write_text(text)\n",
    "\n",
    "print(\"✅ Path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e729e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:15.549080Z",
     "iopub.status.busy": "2026-02-27T07:02:15.548865Z",
     "iopub.status.idle": "2026-02-27T07:02:15.553742Z",
     "shell.execute_reply": "2026-02-27T07:02:15.552996Z"
    },
    "papermill": {
     "duration": 0.011442,
     "end_time": "2026-02-27T07:02:15.555305",
     "exception": false,
     "start_time": "2026-02-27T07:02:15.543863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output paths updated\n"
     ]
    }
   ],
   "source": [
    "text = file_path.read_text()\n",
    "\n",
    "text = text.replace(\n",
    "    'DATA_DIR / \"train.csv\"',\n",
    "    'Path(\"/kaggle/working/train.csv\")'\n",
    ")\n",
    "\n",
    "text = text.replace(\n",
    "    'DATA_DIR / \"val.csv\"',\n",
    "    'Path(\"/kaggle/working/val.csv\")'\n",
    ")\n",
    "\n",
    "file_path.write_text(text)\n",
    "\n",
    "print(\"✅ Output paths updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6c5466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:02:15.564829Z",
     "iopub.status.busy": "2026-02-27T07:02:15.564272Z",
     "iopub.status.idle": "2026-02-27T07:04:10.464929Z",
     "shell.execute_reply": "2026-02-27T07:04:10.464087Z"
    },
    "papermill": {
     "duration": 114.908033,
     "end_time": "2026-02-27T07:04:10.467520",
     "exception": false,
     "start_time": "2026-02-27T07:02:15.559487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/lesion-aware-dr\n",
      "Original columns: ['image', 'level', 'Usage']\r\n",
      "✅ All image files found.\r\n",
      "Saved: /kaggle/working/train.csv\r\n",
      "Saved: /kaggle/working/val.csv\r\n",
      "Train size: 70960\r\n",
      "Val size: 17740\r\n",
      "\r\n",
      "Train label distribution:\r\n",
      "label\r\n",
      "0    52273\r\n",
      "1     4964\r\n",
      "2    10522\r\n",
      "3     1670\r\n",
      "4     1531\r\n",
      "Name: count, dtype: int64\r\n",
      "\r\n",
      "Val label distribution:\r\n",
      "label\r\n",
      "0    13069\r\n",
      "1     1241\r\n",
      "2     2630\r\n",
      "3      417\r\n",
      "4      383\r\n",
      "Name: count, dtype: int64\r\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/lesion-aware-dr\n",
    "!python scripts/make_split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b4ef3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.478529Z",
     "iopub.status.busy": "2026-02-27T07:04:10.477765Z",
     "iopub.status.idle": "2026-02-27T07:04:10.594608Z",
     "shell.execute_reply": "2026-02-27T07:04:10.593840Z"
    },
    "papermill": {
     "duration": 0.124406,
     "end_time": "2026-02-27T07:04:10.596578",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.472172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesion-aware-dr  __notebook__.ipynb  train.csv\tval.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1583f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.608038Z",
     "iopub.status.busy": "2026-02-27T07:04:10.607739Z",
     "iopub.status.idle": "2026-02-27T07:04:10.639326Z",
     "shell.execute_reply": "2026-02-27T07:04:10.638331Z"
    },
    "papermill": {
     "duration": 0.03932,
     "end_time": "2026-02-27T07:04:10.640779",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.601459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patched paths + loss + sampler in base.yaml\n",
      "\n",
      "--- paths block ---\n",
      "paths:\n",
      "  data_dir: \"/kaggle/working\"\n",
      "  raw_dir: \"/kaggle/working\"\n",
      "  processed_dir: \"/kaggle/working\"\n",
      "  outputs_dir: \"/kaggle/working/outputs\"\n",
      "  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n",
      "\n",
      "\n",
      "model:\n",
      "  backbone: \"efficientnet_b0\"\n",
      "  num_classes: 5\n",
      "  pretrained: true\n",
      "\n",
      "data:\n",
      "  use_dummy: false\n",
      "  image_size: 224\n",
      "\n",
      "  train_csv: \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/train.csv\"\n",
      "  val_csv:   \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/val_clean.csv\"\n",
      "  image_dir: \"/content/eyepacs_images/Images\"\n",
      "  image_col: \"image_id\"\n",
      "  label_col: \"label\"\n",
      "\n",
      "  # Use ONE balancing method at a time.\n",
      "  # Here: sampler OFF, loss weights ON.\n",
      "\n",
      "--- data block (sampler lines) ---\n",
      "(no sampler lines found)\n",
      "\n",
      "--- loss block ---\n",
      "loss block not found\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "cfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\n",
    "text = cfg_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# ---------- 1) Force paths block ----------\n",
    "new_paths_block = \"\"\"paths:\n",
    "  data_dir: \"/kaggle/working\"\n",
    "  raw_dir: \"/kaggle/working\"\n",
    "  processed_dir: \"/kaggle/working\"\n",
    "  outputs_dir: \"/kaggle/working/outputs\"\n",
    "  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n",
    "\"\"\"\n",
    "\n",
    "# Replace existing paths block if present, else prepend it\n",
    "if re.search(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", text):\n",
    "    text = re.sub(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", new_paths_block + \"\\n\", text)\n",
    "else:\n",
    "    text = new_paths_block + \"\\n\" + text\n",
    "\n",
    "# ---------- 2) Force loss + sampler settings ----------\n",
    "desired = {\n",
    "    \"loss.name\": \"focal\",\n",
    "    \"loss.gamma\": \"1.5\",\n",
    "    \"loss.use_class_weights\": \"true\",\n",
    "    \"data.use_weighted_sampler\": \"false\",\n",
    "}\n",
    "\n",
    "def set_key(cfg_text: str, key: str, value: str) -> str:\n",
    "    section, field = key.split(\".\")\n",
    "\n",
    "    # Replace field inside existing section block\n",
    "    pattern = rf\"(?ms)^({section}:\\n(?:^[ \\t].*\\n)*)^[ \\t]*{field}:\\s*.*$\"\n",
    "    repl = rf\"\\1  {field}: {value}\"\n",
    "    if re.search(pattern, cfg_text):\n",
    "        return re.sub(pattern, repl, cfg_text)\n",
    "\n",
    "    # If section exists but field missing: insert after section header\n",
    "    sec_header = rf\"(?m)^{section}:\\s*$\"\n",
    "    if re.search(sec_header, cfg_text):\n",
    "        return re.sub(sec_header, f\"{section}:\\n  {field}: {value}\", cfg_text, count=1)\n",
    "\n",
    "    # If section missing entirely: append section at end\n",
    "    return cfg_text.rstrip() + f\"\\n\\n{section}:\\n  {field}: {value}\\n\"\n",
    "\n",
    "for k, v in desired.items():\n",
    "    text = set_key(text, k, v)\n",
    "\n",
    "cfg_path.write_text(text, encoding=\"utf-8\")\n",
    "print(\"✅ Patched paths + loss + sampler in base.yaml\")\n",
    "\n",
    "# ---------- 3) Quick verification print ----------\n",
    "print(\"\\n--- paths block ---\")\n",
    "m = re.search(r\"(?ms)^paths:\\n(?:^[ \\t].*\\n)+\", text)\n",
    "print(m.group(0).strip() if m else \"paths block not found\")\n",
    "\n",
    "print(\"\\n--- data block (sampler lines) ---\")\n",
    "m = re.search(r\"(?ms)^data:\\n(?:^[ \\t].*\\n)+\", text)\n",
    "if m:\n",
    "    # print only key lines\n",
    "    data_lines = [ln for ln in m.group(0).splitlines() if \"use_weighted_sampler\" in ln or \"sampler_mode\" in ln]\n",
    "    print(\"\\n\".join(data_lines) if data_lines else \"(no sampler lines found)\")\n",
    "else:\n",
    "    print(\"data block not found\")\n",
    "\n",
    "print(\"\\n--- loss block ---\")\n",
    "m = re.search(r\"(?ms)^loss:\\n(?:^[ \\t].*\\n)+\", text)\n",
    "print(m.group(0).strip() if m else \"loss block not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb60345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.651199Z",
     "iopub.status.busy": "2026-02-27T07:04:10.650939Z",
     "iopub.status.idle": "2026-02-27T07:04:10.656389Z",
     "shell.execute_reply": "2026-02-27T07:04:10.655769Z"
    },
    "papermill": {
     "duration": 0.012299,
     "end_time": "2026-02-27T07:04:10.657764",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.645465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patched paths block in base.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "cfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\n",
    "text = cfg_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "new_paths_block = \"\"\"paths:\n",
    "  data_dir: \"/kaggle/working\"\n",
    "  raw_dir: \"/kaggle/working\"\n",
    "  processed_dir: \"/kaggle/working\"\n",
    "  outputs_dir: \"/kaggle/working/outputs\"\n",
    "  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n",
    "\"\"\"\n",
    "\n",
    "# Replace existing paths: block if present, else prepend it.\n",
    "if re.search(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", text):\n",
    "    text = re.sub(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", new_paths_block + \"\\n\", text)\n",
    "else:\n",
    "    text = new_paths_block + \"\\n\" + text\n",
    "\n",
    "cfg_path.write_text(text, encoding=\"utf-8\")\n",
    "print(\"✅ Patched paths block in base.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cc5a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.667790Z",
     "iopub.status.busy": "2026-02-27T07:04:10.667557Z",
     "iopub.status.idle": "2026-02-27T07:04:10.803580Z",
     "shell.execute_reply": "2026-02-27T07:04:10.802776Z"
    },
    "papermill": {
     "duration": 0.143039,
     "end_time": "2026-02-27T07:04:10.805396",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.662357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name: \"lesion_aware_dr\"\r\n",
      "run_name: \"efficientnet_cb_focal\"\r\n",
      "\r\n",
      "paths:\r\n",
      "  data_dir: \"/kaggle/working\"\r\n",
      "  raw_dir: \"/kaggle/working\"\r\n",
      "  processed_dir: \"/kaggle/working\"\r\n",
      "  outputs_dir: \"/kaggle/working/outputs\"\r\n",
      "  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "model:\r\n",
      "  backbone: \"efficientnet_b0\"\r\n",
      "  num_classes: 5\r\n",
      "  pretrained: true\r\n",
      "\r\n",
      "data:\r\n",
      "  use_dummy: false\r\n",
      "  image_size: 224\r\n",
      "\r\n",
      "  train_csv: \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/train.csv\"\r\n",
      "  val_csv:   \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/val_clean.csv\"\r\n",
      "  image_dir: \"/content/eyepacs_images/Images\"\r\n",
      "  image_col: \"image_id\"\r\n",
      "  label_col: \"label\"\r\n",
      "\r\n",
      "  # Use ONE balancing method at a time.\r\n",
      "  # Here: sampler OFF, loss weights ON.\r\n",
      "  use_weighted_sampler: false"
     ]
    }
   ],
   "source": [
    "!sed -n '1,120p' /kaggle/working/lesion-aware-dr/configs/base.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec6133bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.816627Z",
     "iopub.status.busy": "2026-02-27T07:04:10.816338Z",
     "iopub.status.idle": "2026-02-27T07:04:10.826707Z",
     "shell.execute_reply": "2026-02-27T07:04:10.825949Z"
    },
    "papermill": {
     "duration": 0.017923,
     "end_time": "2026-02-27T07:04:10.828235",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.810312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ requirements fixed & converted to UTF-8\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "req_path = Path(\"/kaggle/working/lesion-aware-dr/requirements_local.txt\")\n",
    "\n",
    "# try reading with utf-16 fallback\n",
    "try:\n",
    "    text = req_path.read_text(encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    text = req_path.read_text(encoding=\"utf-16\")\n",
    "\n",
    "lines = text.splitlines()\n",
    "\n",
    "remove_keys = [\n",
    "    \"torch==\",\n",
    "    \"torchvision==\",\n",
    "    \"torchaudio==\",\n",
    "    \"opencv-python==\",\n",
    "]\n",
    "\n",
    "cleaned = [line for line in lines if not any(k in line for k in remove_keys)]\n",
    "\n",
    "needed = [\"grad-cam\", \"torchmetrics\", \"einops\"]\n",
    "\n",
    "for pkg in needed:\n",
    "    if not any(pkg in line for line in cleaned):\n",
    "        cleaned.append(pkg)\n",
    "\n",
    "req_path.write_text(\"\\n\".join(cleaned), encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ requirements fixed & converted to UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17039030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.838683Z",
     "iopub.status.busy": "2026-02-27T07:04:10.838431Z",
     "iopub.status.idle": "2026-02-27T07:04:10.848516Z",
     "shell.execute_reply": "2026-02-27T07:04:10.847680Z"
    },
    "papermill": {
     "duration": 0.01703,
     "end_time": "2026-02-27T07:04:10.849983",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.832953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: /kaggle/working/lesion-aware-dr/src/data/dr_datamodule.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo = Path(\"/kaggle/working/lesion-aware-dr\")\n",
    "\n",
    "data_dir = repo / \"src\" / \"data\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# make it a package\n",
    "(data_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "code = r'''from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# ---------- Dummy dataset for quick testing ----------\n",
    "\n",
    "class DummyDRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns random images & labels.\n",
    "    Use only when cfg.data.use_dummy = true.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples: int, num_classes: int, image_size: int):\n",
    "        self.num_samples = int(num_samples)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.image_size = int(image_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        image = torch.rand(3, self.image_size, self.image_size)\n",
    "        label = torch.randint(0, self.num_classes, (1,)).item()\n",
    "        return {\"image\": image, \"label\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "\n",
    "# ---------- Real dataset for APTOS / EyePACS / etc. ----------\n",
    "\n",
    "class DRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generic DR dataset reading from CSV:\n",
    "      - image filename column (e.g., '16028_left.png')\n",
    "      - integer label column (0..num_classes-1)\n",
    "\n",
    "    Robust to corrupted images: retries a few times and then errors clearly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        images_dir: str,\n",
    "        image_col: str,\n",
    "        label_col: str,\n",
    "        image_size: int = 224,\n",
    "        augment: bool = False,\n",
    "        max_decode_retries: int = 20,\n",
    "        log_bad_every: int = 50,\n",
    "    ):\n",
    "        self.df = pd.read_csv(csv_path).reset_index(drop=True)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.image_col = str(image_col)\n",
    "        self.label_col = str(label_col)\n",
    "        self.image_size = int(image_size)\n",
    "\n",
    "        self.max_decode_retries = int(max_decode_retries)\n",
    "        self.log_bad_every = int(log_bad_every)\n",
    "        self.bad_count = 0\n",
    "\n",
    "        # ✅ EfficientNet pretrained expects ImageNet normalization\n",
    "        imagenet_mean = (0.485, 0.456, 0.406)\n",
    "        imagenet_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "        if augment:\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.Resize(image_size, image_size),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.RandomBrightnessContrast(p=0.4),\n",
    "                    # Replace ShiftScaleRotate with Affine (newer albumentations recommendation)\n",
    "                    A.Affine(\n",
    "                        scale=(0.95, 1.05),\n",
    "                        translate_percent=(0.0, 0.05),\n",
    "                        rotate=(-15, 15),\n",
    "                        p=0.5,\n",
    "                        mode=cv2.BORDER_REFLECT_101,\n",
    "                    ),\n",
    "                    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.Resize(image_size, image_size),\n",
    "                    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _read_image(self, img_path: Path):\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            return None\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        for _ in range(self.max_decode_retries):\n",
    "            row = self.df.iloc[idx]\n",
    "            fname = str(row[self.image_col])\n",
    "            img_path = self.images_dir / fname\n",
    "\n",
    "            image = self._read_image(img_path)\n",
    "\n",
    "            if image is not None:\n",
    "                label = int(row[self.label_col])\n",
    "                augmented = self.transform(image=image)\n",
    "                return {\n",
    "                    \"image\": augmented[\"image\"],\n",
    "                    \"label\": torch.tensor(label, dtype=torch.long),\n",
    "                }\n",
    "\n",
    "            # Bad image encountered\n",
    "            self.bad_count += 1\n",
    "            if self.log_bad_every > 0 and self.bad_count % self.log_bad_every == 0:\n",
    "                print(f\"[WARN] Skipped {self.bad_count} corrupted images so far. Latest: {img_path}\")\n",
    "\n",
    "            idx = random.randint(0, len(self.df) - 1)\n",
    "\n",
    "        raise RuntimeError(\n",
    "            f\"Too many unreadable images encountered (>{self.max_decode_retries} retries). \"\n",
    "            f\"Check dataset integrity and image_dir. Last attempted: {img_path}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ---------- DataModule wrapper ----------\n",
    "\n",
    "class DRDataModule:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.batch_size = int(cfg.training.batch_size)\n",
    "        self.num_workers = int(cfg.training.num_workers)\n",
    "        self.num_classes = int(cfg.model.num_classes)\n",
    "        self.image_size = int(cfg.data.image_size)\n",
    "\n",
    "        self.use_dummy = bool(cfg.data.get(\"use_dummy\", False))\n",
    "\n",
    "        # Optional sampler (only if you intentionally enable it in YAML)\n",
    "        self.use_weighted_sampler = bool(cfg.data.get(\"use_weighted_sampler\", False))\n",
    "        self.sampler_mode = str(cfg.data.get(\"sampler_mode\", \"inverse\")).lower()\n",
    "\n",
    "    def _pin_memory(self) -> bool:\n",
    "        return torch.cuda.is_available()\n",
    "\n",
    "    def _build_weighted_sampler_from_csv(self, csv_path: str, label_col: str) -> WeightedRandomSampler:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        labels = df[label_col].astype(int).values\n",
    "\n",
    "        class_counts = np.bincount(labels, minlength=self.num_classes).astype(np.float32)\n",
    "        class_counts = np.maximum(class_counts, 1.0)\n",
    "\n",
    "        # inverse-frequency sampling\n",
    "        class_weights = 1.0 / class_counts\n",
    "        sample_weights = class_weights[labels]\n",
    "        sample_weights = torch.tensor(sample_weights, dtype=torch.double)\n",
    "\n",
    "        return WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True,\n",
    "        )\n",
    "\n",
    "    # ---- Dataloaders ----\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        if self.use_dummy:\n",
    "            ds = DummyDRDataset(\n",
    "                num_samples=256,\n",
    "                num_classes=self.num_classes,\n",
    "                image_size=self.image_size,\n",
    "            )\n",
    "            return DataLoader(\n",
    "                ds,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=self._pin_memory(),\n",
    "            )\n",
    "\n",
    "        ds = DRDataset(\n",
    "            csv_path=self.cfg.data.train_csv,\n",
    "            images_dir=self.cfg.data.image_dir,\n",
    "            image_col=self.cfg.data.image_col,\n",
    "            label_col=self.cfg.data.label_col,\n",
    "            image_size=self.image_size,\n",
    "            augment=True,\n",
    "            max_decode_retries=30,\n",
    "            log_bad_every=50,\n",
    "        )\n",
    "\n",
    "        if self.use_weighted_sampler:\n",
    "            sampler = self._build_weighted_sampler_from_csv(\n",
    "                csv_path=self.cfg.data.train_csv,\n",
    "                label_col=self.cfg.data.label_col,\n",
    "            )\n",
    "            return DataLoader(\n",
    "                ds,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,  # sampler + shuffle cannot both be True\n",
    "                sampler=sampler,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=self._pin_memory(),\n",
    "            )\n",
    "\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self._pin_memory(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        if self.use_dummy:\n",
    "            ds = DummyDRDataset(\n",
    "                num_samples=64,\n",
    "                num_classes=self.num_classes,\n",
    "                image_size=self.image_size,\n",
    "            )\n",
    "        else:\n",
    "            ds = DRDataset(\n",
    "                csv_path=self.cfg.data.val_csv,\n",
    "                images_dir=self.cfg.data.image_dir,\n",
    "                image_col=self.cfg.data.image_col,\n",
    "                label_col=self.cfg.data.label_col,\n",
    "                image_size=self.image_size,\n",
    "                augment=False,\n",
    "                max_decode_retries=30,\n",
    "                log_bad_every=50,\n",
    "            )\n",
    "\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self._pin_memory(),\n",
    "        )\n",
    "'''\n",
    "\n",
    "(data_dir / \"dr_datamodule.py\").write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Created:\", data_dir / \"dr_datamodule.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef23c2d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.860521Z",
     "iopub.status.busy": "2026-02-27T07:04:10.860303Z",
     "iopub.status.idle": "2026-02-27T07:04:10.864671Z",
     "shell.execute_reply": "2026-02-27T07:04:10.864001Z"
    },
    "papermill": {
     "duration": 0.011508,
     "end_time": "2026-02-27T07:04:10.866269",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.854761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albucore==0.0.24\n",
      "albumentations==2.0.8\n",
      "altair==5.5.0\n",
      "annotated-types==0.7.0\n",
      "antlr4-python3-runtime==4.9.3\n",
      "anyio==4.12.0\n",
      "attrs==25.4.0\n",
      "blinker==1.9.0\n",
      "cachetools==6.2.2\n",
      "certifi==2025.11.12\n",
      "charset-norma\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "p = Path(\"/kaggle/working/lesion-aware-dr/requirements_local.txt\")\n",
    "print(p.read_text()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3a78b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:04:10.876422Z",
     "iopub.status.busy": "2026-02-27T07:04:10.876230Z",
     "iopub.status.idle": "2026-02-27T07:05:58.979442Z",
     "shell.execute_reply": "2026-02-27T07:05:58.978498Z"
    },
    "papermill": {
     "duration": 108.110192,
     "end_time": "2026-02-27T07:05:58.981164",
     "exception": false,
     "start_time": "2026-02-27T07:04:10.870972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.1/516.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.0/425.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.3/395.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.1/653.1 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.2/231.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "google-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\r\n",
      "transformers 5.2.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 1.1.6 which is incompatible.\r\n",
      "ydata-profiling 4.18.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\r\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.5.1, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\r\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\r\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\r\n",
      "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\r\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\r\n",
      "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r /kaggle/working/lesion-aware-dr/requirements_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ecee05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:05:58.996966Z",
     "iopub.status.busy": "2026-02-27T07:05:58.996348Z",
     "iopub.status.idle": "2026-02-27T07:06:05.374335Z",
     "shell.execute_reply": "2026-02-27T07:06:05.373487Z"
    },
    "papermill": {
     "duration": 6.387541,
     "end_time": "2026-02-27T07:06:05.375984",
     "exception": false,
     "start_time": "2026-02-27T07:05:58.988443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "673e3d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:06:05.391957Z",
     "iopub.status.busy": "2026-02-27T07:06:05.391083Z",
     "iopub.status.idle": "2026-02-27T07:06:05.397009Z",
     "shell.execute_reply": "2026-02-27T07:06:05.396253Z"
    },
    "papermill": {
     "duration": 0.015346,
     "end_time": "2026-02-27T07:06:05.398540",
     "exception": false,
     "start_time": "2026-02-27T07:06:05.383194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rewrote base.yaml cleanly: /kaggle/working/lesion-aware-dr/configs/base.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\n",
    "\n",
    "clean_yaml = \"\"\"project_name: \"lesion_aware_dr\"\n",
    "run_name: \"efficientnet_focal_g15_cw\"\n",
    "\n",
    "paths:\n",
    "  data_dir: \"/kaggle/working\"\n",
    "  raw_dir: \"/kaggle/working\"\n",
    "  processed_dir: \"/kaggle/working\"\n",
    "  outputs_dir: \"/kaggle/working/outputs\"\n",
    "  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n",
    "\n",
    "model:\n",
    "  backbone: \"efficientnet_b0\"\n",
    "  num_classes: 5\n",
    "  pretrained: true\n",
    "\n",
    "data:\n",
    "  use_dummy: false\n",
    "  image_size: 224\n",
    "\n",
    "  train_csv: \"/kaggle/working/train.csv\"\n",
    "  val_csv:   \"/kaggle/working/val.csv\"\n",
    "  image_dir: \"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\"\n",
    "  image_col: \"image_id\"\n",
    "  label_col: \"label\"\n",
    "\n",
    "  use_weighted_sampler: false\n",
    "  sampler_mode: \"inverse\"\n",
    "\n",
    "training:\n",
    "  seed: 42\n",
    "  epochs: 12\n",
    "  batch_size: 16\n",
    "  num_workers: 2\n",
    "  lr: 1e-4\n",
    "  weight_decay: 1e-5\n",
    "  optimizer: \"adamw\"\n",
    "  scheduler: \"cosine\"\n",
    "\n",
    "logging:\n",
    "  use_wandb: false\n",
    "  wandb_project: \"lesion_aware_dr\"\n",
    "  wandb_entity: null\n",
    "\n",
    "loss:\n",
    "  name: \"cb_focal\"\n",
    "  beta: 0.999\n",
    "  gamma: 2.0\n",
    "  alpha_mode: \"effective\"\n",
    "  reduction: \"mean\"\n",
    "  use_class_weights: false\n",
    "\"\"\"\n",
    "\n",
    "cfg_path.write_text(clean_yaml, encoding=\"utf-8\")\n",
    "print(\"✅ Rewrote base.yaml cleanly:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "615ce911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T07:06:05.414043Z",
     "iopub.status.busy": "2026-02-27T07:06:05.413510Z",
     "iopub.status.idle": "2026-02-27T09:14:35.419670Z",
     "shell.execute_reply": "2026-02-27T09:14:35.418877Z"
    },
    "papermill": {
     "duration": 7710.015949,
     "end_time": "2026-02-27T09:14:35.421578",
     "exception": false,
     "start_time": "2026-02-27T07:06:05.405629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/lesion-aware-dr\n",
      "[2026-02-27 07:06:18] [INFO] Starting training script\r\n",
      "[2026-02-27 07:06:18] [INFO] Using device: cuda\r\n",
      "[2026-02-27 07:06:18] [INFO] [RUN DIR] New run folder: /kaggle/working/outputs/2026-02-27\r\n",
      "[2026-02-27 07:06:18] [INFO] [RUN DIR] Checkpoints folder: /kaggle/working/outputs/2026-02-27/checkpoints\r\n",
      "[2026-02-27 07:06:18] [INFO] Config:\r\n",
      "project_name: lesion_aware_dr\r\n",
      "run_name: efficientnet_focal_g15_cw\r\n",
      "paths:\r\n",
      "  data_dir: /kaggle/working\r\n",
      "  raw_dir: /kaggle/working\r\n",
      "  processed_dir: /kaggle/working\r\n",
      "  outputs_dir: /kaggle/working/outputs/2026-02-27\r\n",
      "  checkpoints_dir: /kaggle/working/outputs/2026-02-27/checkpoints\r\n",
      "model:\r\n",
      "  backbone: efficientnet_b0\r\n",
      "  num_classes: 5\r\n",
      "  pretrained: true\r\n",
      "data:\r\n",
      "  use_dummy: false\r\n",
      "  image_size: 224\r\n",
      "  train_csv: /kaggle/working/train.csv\r\n",
      "  val_csv: /kaggle/working/val.csv\r\n",
      "  image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\r\n",
      "  image_col: image_id\r\n",
      "  label_col: label\r\n",
      "  use_weighted_sampler: false\r\n",
      "  sampler_mode: inverse\r\n",
      "training:\r\n",
      "  seed: 42\r\n",
      "  epochs: 12\r\n",
      "  batch_size: 16\r\n",
      "  num_workers: 2\r\n",
      "  lr: 0.0001\r\n",
      "  weight_decay: 1.0e-05\r\n",
      "  optimizer: adamw\r\n",
      "  scheduler: cosine\r\n",
      "logging:\r\n",
      "  use_wandb: false\r\n",
      "  wandb_project: lesion_aware_dr\r\n",
      "  wandb_entity: null\r\n",
      "loss:\r\n",
      "  name: cb_focal\r\n",
      "  beta: 0.999\r\n",
      "  gamma: 2.0\r\n",
      "  alpha_mode: effective\r\n",
      "  reduction: mean\r\n",
      "  use_class_weights: false\r\n",
      "\r\n",
      "/kaggle/working/lesion-aware-dr/src/data/dr_datamodule.py:81: UserWarning: Argument(s) 'mode' are not valid for transform Affine\r\n",
      "  A.Affine(\r\n",
      "[2026-02-27 07:06:18] [INFO] Train samples: 70960\r\n",
      "[2026-02-27 07:06:18] [INFO] Val samples:   17740\r\n",
      "[2026-02-27 07:06:18] [INFO] Train batches: 4435\r\n",
      "[2026-02-27 07:06:18] [INFO] Val batches:   1109\r\n",
      "model.safetensors: 100%|███████████████████| 21.4M/21.4M [00:00<00:00, 38.1MB/s]\r\n",
      "/kaggle/working/lesion-aware-dr/src/train.py:251: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\r\n",
      "Epoch 1/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 1/12 [TRAIN]:  13%|▏| 593/4435 [01:40<10:54,  5.87it/s, loss=0.1460, lr=1.libpng error: Read Error\r\n",
      "Epoch 1/12 [TRAIN]: 100%|█| 4435/4435 [12:17<00:00,  6.02it/s, loss=0.3706, lr=1\r\n",
      "Epoch 1/12 [VAL]: 100%|████| 1109/1109 [02:56<00:00,  6.28it/s, val_loss=0.1441]\r\n",
      "[2026-02-27 07:21:33] [INFO] Epoch [1/12] train_loss=0.4243 val_loss=0.2904 val_acc=0.7856 val_f1=0.4176 (best_f1=-1.0000, best_loss=inf)\r\n",
      "[2026-02-27 07:21:34] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4176)\r\n",
      "[2026-02-27 07:21:34] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2904)\r\n",
      "Epoch 2/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 2/12 [TRAIN]:  93%|▉| 4136/4435 [07:41<00:32,  9.10it/s, loss=0.2230, lr=9libpng error: Read Error\r\n",
      "Epoch 2/12 [TRAIN]: 100%|█| 4435/4435 [08:14<00:00,  8.97it/s, loss=0.2835, lr=9\r\n",
      "Epoch 2/12 [VAL]: 100%|████| 1109/1109 [01:48<00:00, 10.22it/s, val_loss=0.1519]\r\n",
      "[2026-02-27 07:31:37] [INFO] Epoch [2/12] train_loss=0.2834 val_loss=0.2634 val_acc=0.8005 val_f1=0.4692 (best_f1=0.4176, best_loss=0.2904)\r\n",
      "[2026-02-27 07:31:37] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4692)\r\n",
      "[2026-02-27 07:31:37] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2634)\r\n",
      "Epoch 3/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 3/12 [TRAIN]:   3%| | 146/4435 [00:16<08:05,  8.84it/s, loss=0.1344, lr=9.libpng error: Read Error\r\n",
      "Epoch 3/12 [TRAIN]: 100%|█| 4435/4435 [08:16<00:00,  8.94it/s, loss=0.1327, lr=9\r\n",
      "Epoch 3/12 [VAL]: 100%|████| 1109/1109 [01:48<00:00, 10.20it/s, val_loss=0.1836]\r\n",
      "[2026-02-27 07:41:42] [INFO] Epoch [3/12] train_loss=0.2578 val_loss=0.2545 val_acc=0.7998 val_f1=0.4920 (best_f1=0.4692, best_loss=0.2634)\r\n",
      "[2026-02-27 07:41:42] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4920)\r\n",
      "[2026-02-27 07:41:42] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2545)\r\n",
      "Epoch 4/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 4/12 [TRAIN]:  63%|▋| 2784/4435 [05:14<03:06,  8.84it/s, loss=0.1753, lr=8libpng error: Read Error\r\n",
      "Epoch 4/12 [TRAIN]: 100%|█| 4435/4435 [08:23<00:00,  8.82it/s, loss=0.1661, lr=8\r\n",
      "Epoch 4/12 [VAL]: 100%|████| 1109/1109 [01:52<00:00,  9.82it/s, val_loss=0.1409]\r\n",
      "[2026-02-27 07:51:58] [INFO] Epoch [4/12] train_loss=0.2379 val_loss=0.2424 val_acc=0.8118 val_f1=0.5022 (best_f1=0.4920, best_loss=0.2545)\r\n",
      "[2026-02-27 07:51:59] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5022)\r\n",
      "[2026-02-27 07:51:59] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2424)\r\n",
      "Epoch 5/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 5/12 [TRAIN]:  90%|▉| 4000/4435 [07:46<00:47,  9.20it/s, loss=0.1069, lr=7libpng error: Read Error\r\n",
      "Epoch 5/12 [TRAIN]: 100%|█| 4435/4435 [08:36<00:00,  8.59it/s, loss=0.1661, lr=7\r\n",
      "Epoch 5/12 [VAL]: 100%|████| 1109/1109 [01:52<00:00,  9.84it/s, val_loss=0.1924]\r\n",
      "[2026-02-27 08:02:28] [INFO] Epoch [5/12] train_loss=0.2217 val_loss=0.2437 val_acc=0.8148 val_f1=0.4953 (best_f1=0.5022, best_loss=0.2424)\r\n",
      "Epoch 6/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 6/12 [TRAIN]:  39%|▍| 1751/4435 [03:26<05:16,  8.48it/s, loss=0.1336, lr=6libpng error: Read Error\r\n",
      "Epoch 6/12 [TRAIN]: 100%|█| 4435/4435 [08:26<00:00,  8.75it/s, loss=0.1480, lr=6\r\n",
      "Epoch 6/12 [VAL]: 100%|████| 1109/1109 [01:42<00:00, 10.80it/s, val_loss=0.2022]\r\n",
      "[2026-02-27 08:12:37] [INFO] Epoch [6/12] train_loss=0.2049 val_loss=0.2527 val_acc=0.8109 val_f1=0.4637 (best_f1=0.5022, best_loss=0.2424)\r\n",
      "Epoch 7/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 7/12 [TRAIN]:  41%|▍| 1824/4435 [03:27<04:59,  8.73it/s, loss=0.1698, lr=5libpng error: Read Error\r\n",
      "Epoch 7/12 [TRAIN]: 100%|█| 4435/4435 [08:21<00:00,  8.84it/s, loss=0.0955, lr=5\r\n",
      "Epoch 7/12 [VAL]: 100%|████| 1109/1109 [01:49<00:00, 10.10it/s, val_loss=0.1577]\r\n",
      "[2026-02-27 08:22:49] [INFO] Epoch [7/12] train_loss=0.1862 val_loss=0.2597 val_acc=0.8005 val_f1=0.5019 (best_f1=0.5022, best_loss=0.2424)\r\n",
      "Epoch 8/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 8/12 [TRAIN]:   7%| | 305/4435 [00:34<07:50,  8.78it/s, loss=0.1818, lr=3.libpng error: Read Error\r\n",
      "Epoch 8/12 [TRAIN]: 100%|█| 4435/4435 [08:25<00:00,  8.77it/s, loss=0.1688, lr=3\r\n",
      "Epoch 8/12 [VAL]: 100%|████| 1109/1109 [01:49<00:00, 10.13it/s, val_loss=0.1450]\r\n",
      "[2026-02-27 08:33:05] [INFO] Epoch [8/12] train_loss=0.1673 val_loss=0.2582 val_acc=0.8065 val_f1=0.5182 (best_f1=0.5022, best_loss=0.2424)\r\n",
      "[2026-02-27 08:33:05] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5182)\r\n",
      "Epoch 9/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 9/12 [TRAIN]:  42%|▍| 1850/4435 [03:33<04:41,  9.19it/s, loss=0.0549, lr=2libpng error: Read Error\r\n",
      "Epoch 9/12 [TRAIN]: 100%|█| 4435/4435 [08:33<00:00,  8.64it/s, loss=0.0522, lr=2\r\n",
      "Epoch 9/12 [VAL]: 100%|████| 1109/1109 [01:48<00:00, 10.19it/s, val_loss=0.1402]\r\n",
      "[2026-02-27 08:43:27] [INFO] Epoch [9/12] train_loss=0.1511 val_loss=0.2742 val_acc=0.8033 val_f1=0.5162 (best_f1=0.5182, best_loss=0.2424)\r\n",
      "Epoch 10/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 10/12 [TRAIN]:  60%|▌| 2660/4435 [05:03<03:17,  9.00it/s, loss=0.0320, lr=libpng error: Read Error\r\n",
      "Epoch 10/12 [TRAIN]: 100%|█| 4435/4435 [08:25<00:00,  8.77it/s, loss=0.1100, lr=\r\n",
      "Epoch 10/12 [VAL]: 100%|███| 1109/1109 [01:48<00:00, 10.24it/s, val_loss=0.1419]\r\n",
      "[2026-02-27 08:53:41] [INFO] Epoch [10/12] train_loss=0.1359 val_loss=0.2820 val_acc=0.8045 val_f1=0.5184 (best_f1=0.5182, best_loss=0.2424)\r\n",
      "[2026-02-27 08:53:42] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5184)\r\n",
      "Epoch 11/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "libpng error: Read Error\r\n",
      "Epoch 11/12 [TRAIN]: 100%|█| 4435/4435 [08:21<00:00,  8.85it/s, loss=0.0353, lr=\r\n",
      "Epoch 11/12 [VAL]: 100%|███| 1109/1109 [01:45<00:00, 10.52it/s, val_loss=0.1495]\r\n",
      "[2026-02-27 09:03:48] [INFO] Epoch [11/12] train_loss=0.1264 val_loss=0.2932 val_acc=0.8015 val_f1=0.5136 (best_f1=0.5184, best_loss=0.2424)\r\n",
      "Epoch 12/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\r\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\r\n",
      "Epoch 12/12 [TRAIN]:  43%|▍| 1893/4435 [03:37<05:08,  8.23it/s, loss=0.1045, lr=libpng error: Read Error\r\n",
      "Epoch 12/12 [TRAIN]: 100%|█| 4435/4435 [08:48<00:00,  8.40it/s, loss=0.1019, lr=\r\n",
      "Epoch 12/12 [VAL]: 100%|███| 1109/1109 [01:53<00:00,  9.73it/s, val_loss=0.1563]\r\n",
      "[2026-02-27 09:14:30] [INFO] Epoch [12/12] train_loss=0.1212 val_loss=0.2932 val_acc=0.8009 val_f1=0.5276 (best_f1=0.5184, best_loss=0.2424)\r\n",
      "[2026-02-27 09:14:31] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5276)\r\n",
      "[2026-02-27 09:14:31] [INFO] Training completed. Run saved at: /kaggle/working/outputs/2026-02-27\r\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/lesion-aware-dr\n",
    "!python -m src.train --cfg_path configs/base.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154f5121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:14:40.880144Z",
     "iopub.status.busy": "2026-02-27T09:14:40.879470Z",
     "iopub.status.idle": "2026-02-27T09:14:40.888726Z",
     "shell.execute_reply": "2026-02-27T09:14:40.887980Z"
    },
    "papermill": {
     "duration": 2.732298,
     "end_time": "2026-02-27T09:14:40.890290",
     "exception": false,
     "start_time": "2026-02-27T09:14:38.157992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote: /kaggle/working/lesion-aware-dr/src/gradcam_eval.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path(\"/kaggle/working/lesion-aware-dr/src/gradcam_eval.py\")\n",
    "\n",
    "clean_py = r\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "from src.models import build_model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (same CSV format as eval)\n",
    "# -----------------------------\n",
    "class EyePacsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        image_dir: str,\n",
    "        image_col: str,\n",
    "        label_col: str,\n",
    "        transform=None,\n",
    "    ):\n",
    "        import pandas as pd\n",
    "\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_dir = str(image_dir)\n",
    "        self.image_col = str(image_col)\n",
    "        self.label_col = str(label_col)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        row = self.df.iloc[idx]\n",
    "        fname = str(row[self.image_col])\n",
    "        label = int(row[self.label_col])\n",
    "\n",
    "        img_path = str(Path(self.image_dir) / fname)\n",
    "        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise FileNotFoundError(f\"Could not read image: {img_path}\")\n",
    "\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(image=rgb)[\"image\"]\n",
    "        else:\n",
    "            x = torch.from_numpy(rgb).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        return {\n",
    "            \"image\": x,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"fname\": fname,\n",
    "            \"rgb\": rgb,  # uint8 RGB for overlay\n",
    "        }\n",
    "\n",
    "\n",
    "def load_cfg(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def build_transform(image_size: int) -> A.Compose:\n",
    "    # ImageNet normalization (matches EfficientNet pretrained)\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def load_checkpoint(model: torch.nn.Module, ckpt_path: str, device: torch.device) -> None:\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Your train saves {\"model_state\": ...}\n",
    "    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "        state = ckpt[\"model_state\"]\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    elif isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    else:\n",
    "        state = ckpt\n",
    "\n",
    "    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
    "    model.load_state_dict(state, strict=True)\n",
    "\n",
    "\n",
    "def find_last_conv_layer(model: torch.nn.Module) -> torch.nn.Module:\n",
    "    last_conv = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            last_conv = m\n",
    "    if last_conv is None:\n",
    "        raise RuntimeError(\"No Conv2d found. Can't run Grad-CAM.\")\n",
    "    return last_conv\n",
    "\n",
    "\n",
    "def overlay_cam(rgb_uint8: np.ndarray, cam_2d: np.ndarray, alpha: float = 0.45) -> np.ndarray:\n",
    "    h, w = rgb_uint8.shape[:2]\n",
    "    cam_resized = cv2.resize(cam_2d, (w, h))\n",
    "    cam_uint8 = np.uint8(255 * np.clip(cam_resized, 0, 1))\n",
    "\n",
    "    heatmap_bgr = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n",
    "    heatmap_rgb = cv2.cvtColor(heatmap_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    out = (1 - alpha) * rgb_uint8.astype(np.float32) + alpha * heatmap_rgb.astype(np.float32)\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "\n",
    "    # Keep BOTH styles: --cfg_path/--ckpt_path and --cfg/--ckpt\n",
    "    ap.add_argument(\"--cfg_path\", type=str, default=None)\n",
    "    ap.add_argument(\"--ckpt_path\", type=str, default=None)\n",
    "    ap.add_argument(\"--cfg\", type=str, default=None)\n",
    "    ap.add_argument(\"--ckpt\", type=str, default=None)\n",
    "\n",
    "    ap.add_argument(\"--split\", type=str, default=\"val\", choices=[\"val\", \"train\"])\n",
    "    ap.add_argument(\"--method\", type=str, default=\"gradcampp\", choices=[\"gradcam\", \"gradcampp\"])\n",
    "    ap.add_argument(\"--target\", type=str, default=\"pred\", choices=[\"pred\", \"true\"])\n",
    "\n",
    "    ap.add_argument(\"--num_images\", type=int, default=30)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    ap.add_argument(\"--num_workers\", type=int, default=0)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    ap.add_argument(\"--alpha\", type=float, default=0.45)\n",
    "\n",
    "    return ap.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    cfg_path = args.cfg_path or args.cfg\n",
    "    ckpt_path = args.ckpt_path or args.ckpt\n",
    "\n",
    "    if not cfg_path or not ckpt_path:\n",
    "        raise ValueError(\"Provide config + checkpoint: use --cfg_path/--ckpt_path OR --cfg/--ckpt\")\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    cfg = load_cfg(cfg_path)\n",
    "\n",
    "    backbone = cfg[\"model\"][\"backbone\"]\n",
    "    num_classes = int(cfg[\"model\"][\"num_classes\"])\n",
    "\n",
    "    image_size = int(cfg[\"data\"][\"image_size\"])\n",
    "    image_dir = cfg[\"data\"][\"image_dir\"]\n",
    "    image_col = cfg[\"data\"][\"image_col\"]\n",
    "    label_col = cfg[\"data\"][\"label_col\"]\n",
    "\n",
    "    csv_path = cfg[\"data\"][\"val_csv\"] if args.split == \"val\" else cfg[\"data\"][\"train_csv\"]\n",
    "\n",
    "    # IMPORTANT: your training now changes cfg.paths.outputs_dir to a RUN folder.\n",
    "    # So Grad-CAM outputs will go into THAT run folder automatically.\n",
    "    outputs_dir = Path(cfg[\"paths\"][\"outputs_dir\"])\n",
    "    out_dir = outputs_dir / \"gradcam\" / f\"{args.method}_{args.split}_{args.target}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[GradCAM] device: {device}\")\n",
    "    print(f\"[GradCAM] csv: {csv_path}\")\n",
    "    print(f\"[GradCAM] image_dir: {image_dir}\")\n",
    "    print(f\"[GradCAM] ckpt: {ckpt_path}\")\n",
    "    print(f\"[GradCAM] out_dir: {out_dir}\")\n",
    "\n",
    "    tfm = build_transform(image_size)\n",
    "    ds = EyePacsDataset(csv_path, image_dir, image_col, label_col, transform=tfm)\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=(device.type == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    # Same model builder as training\n",
    "    model = build_model(backbone=backbone, num_classes=num_classes, pretrained=False).to(device)\n",
    "    load_checkpoint(model, ckpt_path, device)\n",
    "    model.eval()\n",
    "\n",
    "    target_layer = find_last_conv_layer(model)\n",
    "    print(f\"[GradCAM] Using target layer: {target_layer}\")\n",
    "\n",
    "    cam_class = GradCAMPlusPlus if args.method == \"gradcampp\" else GradCAM\n",
    "    cam = cam_class(model=model, target_layers=[target_layer])\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    exported = 0\n",
    "    for batch in dl:\n",
    "        if exported >= args.num_images:\n",
    "            break\n",
    "\n",
    "        xb = batch[\"image\"].to(device, non_blocking=True)\n",
    "        yb = batch[\"label\"].cpu().numpy()\n",
    "        names = batch[\"fname\"]\n",
    "        rgbs = batch[\"rgb\"].numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(xb)\n",
    "            probs = softmax(logits)\n",
    "            pred = torch.argmax(probs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "        target_classes = pred if args.target == \"pred\" else yb\n",
    "        targets = [ClassifierOutputTarget(int(c)) for c in target_classes]\n",
    "\n",
    "        # Grad-CAM needs gradients => no no_grad here\n",
    "        cams = cam(input_tensor=xb, targets=targets)  # (B, H, W)\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            if exported >= args.num_images:\n",
    "                break\n",
    "\n",
    "            rgb = rgbs[i]\n",
    "            if rgb.dtype != np.uint8:\n",
    "                rgb = np.clip(rgb, 0, 255).astype(np.uint8)\n",
    "\n",
    "            overlay = overlay_cam(rgb, cams[i], alpha=float(args.alpha))\n",
    "\n",
    "            out_name = (\n",
    "                f\"{exported:04d}__true{yb[i]}__pred{pred[i]}__target{target_classes[i]}__{Path(names[i]).stem}.png\"\n",
    "            )\n",
    "            out_path = out_dir / out_name\n",
    "\n",
    "            cv2.imwrite(str(out_path), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "            exported += 1\n",
    "\n",
    "    print(f\"[GradCAM] Done. Exported {exported} images to: {out_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "cfg_path.write_text(clean_py, encoding=\"utf-8\")\n",
    "print(\"✅ Wrote:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e6d309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:14:46.337854Z",
     "iopub.status.busy": "2026-02-27T09:14:46.337521Z",
     "iopub.status.idle": "2026-02-27T09:14:46.601188Z",
     "shell.execute_reply": "2026-02-27T09:14:46.600475Z"
    },
    "papermill": {
     "duration": 2.945915,
     "end_time": "2026-02-27T09:14:46.602937",
     "exception": false,
     "start_time": "2026-02-27T09:14:43.657022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3: No module named src.eval_model\r\n"
     ]
    }
   ],
   "source": [
    "!python -m src.eval_model \\\n",
    "--cfg configs/base.yaml \\\n",
    "--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0803a667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:14:51.869560Z",
     "iopub.status.busy": "2026-02-27T09:14:51.868759Z",
     "iopub.status.idle": "2026-02-27T09:14:52.083240Z",
     "shell.execute_reply": "2026-02-27T09:14:52.082457Z"
    },
    "papermill": {
     "duration": 2.849004,
     "end_time": "2026-02-27T09:14:52.084927",
     "exception": false,
     "start_time": "2026-02-27T09:14:49.235923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3: No module named src.eval_model\r\n"
     ]
    }
   ],
   "source": [
    "!python -m src.eval_model \\\n",
    "--cfg configs/base.yaml \\\n",
    "--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_val_loss_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29cfc079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:14:57.526847Z",
     "iopub.status.busy": "2026-02-27T09:14:57.526506Z",
     "iopub.status.idle": "2026-02-27T09:14:57.740481Z",
     "shell.execute_reply": "2026-02-27T09:14:57.739764Z"
    },
    "papermill": {
     "duration": 2.871675,
     "end_time": "2026-02-27T09:14:57.742142",
     "exception": false,
     "start_time": "2026-02-27T09:14:54.870467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3: No module named src.eval_model\r\n"
     ]
    }
   ],
   "source": [
    "!python -m src.eval_model \\\n",
    "--cfg configs/base.yaml \\\n",
    "--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/last_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b43b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-27T09:15:03.175414Z",
     "iopub.status.busy": "2026-02-27T09:15:03.175105Z",
     "iopub.status.idle": "2026-02-27T09:15:14.435424Z",
     "shell.execute_reply": "2026-02-27T09:15:14.434473Z"
    },
    "papermill": {
     "duration": 14.042057,
     "end_time": "2026-02-27T09:15:14.437244",
     "exception": false,
     "start_time": "2026-02-27T09:15:00.395187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradCAM] device: cuda\r\n",
      "[GradCAM] csv: /kaggle/working/val.csv\r\n",
      "[GradCAM] image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\r\n",
      "[GradCAM] ckpt: /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt\r\n",
      "[GradCAM] out_dir: /kaggle/working/outputs/gradcam/gradcampp_val_pred\r\n",
      "[GradCAM] Using target layer: Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "[GradCAM] Done. Exported 30 images to: /kaggle/working/outputs/gradcam/gradcampp_val_pred\r\n"
     ]
    }
   ],
   "source": [
    "!python -m src.gradcam_eval \\\n",
    "  --cfg configs/base.yaml \\\n",
    "  --ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt \\\n",
    "  --split val \\\n",
    "  --method gradcampp \\\n",
    "  --target pred \\\n",
    "  --num_images 30 \\\n",
    "  --batch_size 16 \\\n",
    "  --num_workers 0"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13032739,
     "datasetId": 7859837,
     "sourceId": 12459609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31286,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8005.085279,
   "end_time": "2026-02-27T09:15:17.877172",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-27T07:01:52.791893",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
