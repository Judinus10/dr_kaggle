{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":12459609,"datasetId":7859837,"databundleVersionId":13032739}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":7003.319189,"end_time":"2026-02-25T07:41:41.519530","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-25T05:44:58.200341","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4679312f","cell_type":"code","source":"!nvidia-smi","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2026-02-27T05:56:51.603215Z","iopub.execute_input":"2026-02-27T05:56:51.603588Z","iopub.status.idle":"2026-02-27T05:56:52.076559Z","shell.execute_reply.started":"2026-02-27T05:56:51.603517Z","shell.execute_reply":"2026-02-27T05:56:52.075703Z"},"papermill":{"duration":0.489787,"end_time":"2026-02-25T05:45:01.199254","exception":false,"start_time":"2026-02-25T05:45:00.709467","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Fri Feb 27 05:56:51 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   32C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"id":"e9b17160","cell_type":"code","source":" !git clone https://github.com/Judinus10/lesion-aware-dr.git \n%cd lesion-aware-dr","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:56:52.077800Z","iopub.execute_input":"2026-02-27T05:56:52.078021Z","iopub.status.idle":"2026-02-27T05:56:52.785390Z","shell.execute_reply.started":"2026-02-27T05:56:52.077998Z","shell.execute_reply":"2026-02-27T05:56:52.784517Z"},"papermill":{"duration":0.692208,"end_time":"2026-02-25T05:45:01.894565","exception":false,"start_time":"2026-02-25T05:45:01.202357","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'lesion-aware-dr'...\nremote: Enumerating objects: 201, done.\u001b[K\nremote: Counting objects: 100% (201/201), done.\u001b[K\nremote: Compressing objects: 100% (109/109), done.\u001b[K\nremote: Total 201 (delta 94), reused 178 (delta 71), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (201/201), 103.95 KiB | 2.97 MiB/s, done.\nResolving deltas: 100% (94/94), done.\n/kaggle/working/lesion-aware-dr\n","output_type":"stream"}],"execution_count":3},{"id":"ba02838b","cell_type":"code","source":"DATA_ROOT=\"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\"\n\n!echo $DATA_ROOT\n!ls -lah $DATA_ROOT\n!ls -lah $DATA_ROOT/Images | head","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:56:52.787884Z","iopub.execute_input":"2026-02-27T05:56:52.788325Z","iopub.status.idle":"2026-02-27T05:57:21.527739Z","shell.execute_reply.started":"2026-02-27T05:56:52.788288Z","shell.execute_reply":"2026-02-27T05:57:21.527003Z"},"papermill":{"duration":25.470493,"end_time":"2026-02-25T05:45:27.368471","exception":false,"start_time":"2026-02-25T05:45:01.897978","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\ntotal 3.2M\ndrwxr-xr-x 3 nobody nogroup    0 Feb 16 16:26 .\ndrwxr-xr-x 3 root   root    4.0K Feb 27 05:56 ..\n-rw-r--r-- 1 nobody nogroup 1.6M Feb 16 16:26 all_labels.csv\ndrwxr-xr-x 2 nobody nogroup    0 Feb 16 16:26 Images\n-rw-r--r-- 1 nobody nogroup 1.2M Feb 16 16:26 original_test_labels.csv\n-rw-r--r-- 1 nobody nogroup 455K Feb 16 16:26 original_train_labels.csv\ntotal 23G\ndrwxr-xr-x 2 nobody nogroup    0 Feb 16 16:26 .\ndrwxr-xr-x 3 nobody nogroup    0 Feb 16 16:26 ..\n-rw-r--r-- 1 nobody nogroup 218K Feb 16 16:19 10000_left.png\n-rw-r--r-- 1 nobody nogroup 214K Feb 16 16:19 10000_right.png\n-rw-r--r-- 1 nobody nogroup 216K Feb 16 16:19 10001_left.png\n-rw-r--r-- 1 nobody nogroup 211K Feb 16 16:19 10001_right.png\n-rw-r--r-- 1 nobody nogroup 186K Feb 16 16:19 10002_left.png\n-rw-r--r-- 1 nobody nogroup 297K Feb 16 16:19 10002_right.png\n-rw-r--r-- 1 nobody nogroup 210K Feb 16 16:19 10003_left.png\nls: write error: Broken pipe\n","output_type":"stream"}],"execution_count":4},{"id":"fbbd3788","cell_type":"code","source":"from pathlib import Path\n\nfile_path = Path(\"/kaggle/working/lesion-aware-dr/scripts/make_split.py\")\n\nprint(file_path.read_text()[:500])","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:57:21.528860Z","iopub.execute_input":"2026-02-27T05:57:21.529090Z","iopub.status.idle":"2026-02-27T05:57:21.534336Z","shell.execute_reply.started":"2026-02-27T05:57:21.529065Z","shell.execute_reply":"2026-02-27T05:57:21.533720Z"},"papermill":{"duration":0.011109,"end_time":"2026-02-25T05:45:27.383320","exception":false,"start_time":"2026-02-25T05:45:27.372211","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\n# --------------------\n# Paths\n# --------------------\nDATA_DIR = Path(r\"E:\\DR_related\\eyepacs\")\nLABELS_CSV = DATA_DIR / \"all_labels.csv\"\nIMAGES_DIR = DATA_DIR / \"Images\"\n\nOUT_TRAIN = DATA_DIR / \"train.csv\"\nOUT_VAL = DATA_DIR / \"val.csv\"\n\n# --------------------\n# Load labels\n# --------------------\ndf = pd.read_csv(LABELS_CSV)\nprint(\"Original columns:\", list(df.columns))\n\n# --------------------\n# Nor\n","output_type":"stream"}],"execution_count":5},{"id":"89b539c0","cell_type":"code","source":"from pathlib import Path\n\nfile_path = Path(\"/kaggle/working/lesion-aware-dr/scripts/make_split.py\")\n\ntext = file_path.read_text()\n\ntext = text.replace(\n    r\"E:\\DR_related\\eyepacs\",\n    \"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset\"\n)\n\nfile_path.write_text(text)\n\nprint(\"✅ Path updated\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:57:21.535331Z","iopub.execute_input":"2026-02-27T05:57:21.535637Z","iopub.status.idle":"2026-02-27T05:57:21.546899Z","shell.execute_reply.started":"2026-02-27T05:57:21.535615Z","shell.execute_reply":"2026-02-27T05:57:21.546234Z"},"papermill":{"duration":0.010314,"end_time":"2026-02-25T05:45:27.396994","exception":false,"start_time":"2026-02-25T05:45:27.386680","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Path updated\n","output_type":"stream"}],"execution_count":6},{"id":"7c18363d","cell_type":"code","source":"text = file_path.read_text()\n\ntext = text.replace(\n    'DATA_DIR / \"train.csv\"',\n    'Path(\"/kaggle/working/train.csv\")'\n)\n\ntext = text.replace(\n    'DATA_DIR / \"val.csv\"',\n    'Path(\"/kaggle/working/val.csv\")'\n)\n\nfile_path.write_text(text)\n\nprint(\"✅ Output paths updated\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:57:21.547874Z","iopub.execute_input":"2026-02-27T05:57:21.548106Z","iopub.status.idle":"2026-02-27T05:57:21.559424Z","shell.execute_reply.started":"2026-02-27T05:57:21.548086Z","shell.execute_reply":"2026-02-27T05:57:21.558799Z"},"papermill":{"duration":0.010165,"end_time":"2026-02-25T05:45:27.410682","exception":false,"start_time":"2026-02-25T05:45:27.400517","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Output paths updated\n","output_type":"stream"}],"execution_count":7},{"id":"7e964ea2","cell_type":"code","source":"%cd /kaggle/working/lesion-aware-dr\n!python scripts/make_split.py","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:57:21.560483Z","iopub.execute_input":"2026-02-27T05:57:21.560797Z","iopub.status.idle":"2026-02-27T05:58:09.252384Z","shell.execute_reply.started":"2026-02-27T05:57:21.560770Z","shell.execute_reply":"2026-02-27T05:58:09.251664Z"},"papermill":{"duration":59.997138,"end_time":"2026-02-25T05:46:27.411301","exception":false,"start_time":"2026-02-25T05:45:27.414163","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/lesion-aware-dr\nOriginal columns: ['image', 'level', 'Usage']\n✅ All image files found.\nSaved: /kaggle/working/train.csv\nSaved: /kaggle/working/val.csv\nTrain size: 70960\nVal size: 17740\n\nTrain label distribution:\nlabel\n0    52273\n1     4964\n2    10522\n3     1670\n4     1531\nName: count, dtype: int64\n\nVal label distribution:\nlabel\n0    13069\n1     1241\n2     2630\n3      417\n4      383\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"id":"b353990d","cell_type":"code","source":"!ls /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.257965Z","iopub.execute_input":"2026-02-27T05:58:09.258235Z","iopub.status.idle":"2026-02-27T05:58:09.373835Z","shell.execute_reply.started":"2026-02-27T05:58:09.258210Z","shell.execute_reply":"2026-02-27T05:58:09.372776Z"},"papermill":{"duration":0.122363,"end_time":"2026-02-25T05:46:27.537546","exception":false,"start_time":"2026-02-25T05:46:27.415183","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"lesion-aware-dr  train.csv  val.csv\n","output_type":"stream"}],"execution_count":9},{"id":"01fd553b","cell_type":"code","source":"from pathlib import Path\nimport re\n\ncfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\ntext = cfg_path.read_text(encoding=\"utf-8\")\n\n# ---------- 1) Force paths block ----------\nnew_paths_block = \"\"\"paths:\n  data_dir: \"/kaggle/working\"\n  raw_dir: \"/kaggle/working\"\n  processed_dir: \"/kaggle/working\"\n  outputs_dir: \"/kaggle/working/outputs\"\n  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n\"\"\"\n\n# Replace existing paths block if present, else prepend it\nif re.search(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", text):\n    text = re.sub(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", new_paths_block + \"\\n\", text)\nelse:\n    text = new_paths_block + \"\\n\" + text\n\n# ---------- 2) Force loss + sampler settings ----------\ndesired = {\n    \"loss.name\": \"focal\",\n    \"loss.gamma\": \"1.5\",\n    \"loss.use_class_weights\": \"true\",\n    \"data.use_weighted_sampler\": \"false\",\n}\n\ndef set_key(cfg_text: str, key: str, value: str) -> str:\n    section, field = key.split(\".\")\n\n    # Replace field inside existing section block\n    pattern = rf\"(?ms)^({section}:\\n(?:^[ \\t].*\\n)*)^[ \\t]*{field}:\\s*.*$\"\n    repl = rf\"\\1  {field}: {value}\"\n    if re.search(pattern, cfg_text):\n        return re.sub(pattern, repl, cfg_text)\n\n    # If section exists but field missing: insert after section header\n    sec_header = rf\"(?m)^{section}:\\s*$\"\n    if re.search(sec_header, cfg_text):\n        return re.sub(sec_header, f\"{section}:\\n  {field}: {value}\", cfg_text, count=1)\n\n    # If section missing entirely: append section at end\n    return cfg_text.rstrip() + f\"\\n\\n{section}:\\n  {field}: {value}\\n\"\n\nfor k, v in desired.items():\n    text = set_key(text, k, v)\n\ncfg_path.write_text(text, encoding=\"utf-8\")\nprint(\"✅ Patched paths + loss + sampler in base.yaml\")\n\n# ---------- 3) Quick verification print ----------\nprint(\"\\n--- paths block ---\")\nm = re.search(r\"(?ms)^paths:\\n(?:^[ \\t].*\\n)+\", text)\nprint(m.group(0).strip() if m else \"paths block not found\")\n\nprint(\"\\n--- data block (sampler lines) ---\")\nm = re.search(r\"(?ms)^data:\\n(?:^[ \\t].*\\n)+\", text)\nif m:\n    # print only key lines\n    data_lines = [ln for ln in m.group(0).splitlines() if \"use_weighted_sampler\" in ln or \"sampler_mode\" in ln]\n    print(\"\\n\".join(data_lines) if data_lines else \"(no sampler lines found)\")\nelse:\n    print(\"data block not found\")\n\nprint(\"\\n--- loss block ---\")\nm = re.search(r\"(?ms)^loss:\\n(?:^[ \\t].*\\n)+\", text)\nprint(m.group(0).strip() if m else \"loss block not found\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.407387Z","iopub.execute_input":"2026-02-27T05:58:09.407797Z","iopub.status.idle":"2026-02-27T05:58:09.423468Z","shell.execute_reply.started":"2026-02-27T05:58:09.407767Z","shell.execute_reply":"2026-02-27T05:58:09.422831Z"},"papermill":{"duration":0.03722,"end_time":"2026-02-25T05:46:27.578527","exception":false,"start_time":"2026-02-25T05:46:27.541307","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Patched paths + loss + sampler in base.yaml\n\n--- paths block ---\npaths:\n  data_dir: \"/kaggle/working\"\n  raw_dir: \"/kaggle/working\"\n  processed_dir: \"/kaggle/working\"\n  outputs_dir: \"/kaggle/working/outputs\"\n  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n\n\n\nmodel:\n  backbone: \"efficientnet_b0\"\n  num_classes: 5\n  pretrained: true\n\ndata:\n  use_dummy: false\n  image_size: 224\n\n  train_csv: \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/train.csv\"\n  val_csv:   \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/val_clean.csv\"\n  image_dir: \"/content/eyepacs_images/Images\"\n  image_col: \"image_id\"\n  label_col: \"label\"\n\n  # Use ONE balancing method at a time.\n  # Here: sampler OFF, loss weights ON.\n\n--- data block (sampler lines) ---\n(no sampler lines found)\n\n--- loss block ---\nloss block not found\n","output_type":"stream"}],"execution_count":11},{"id":"1c222ccd","cell_type":"code","source":"from pathlib import Path\nimport re\n\ncfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\ntext = cfg_path.read_text(encoding=\"utf-8\")\n\nnew_paths_block = \"\"\"paths:\n  data_dir: \"/kaggle/working\"\n  raw_dir: \"/kaggle/working\"\n  processed_dir: \"/kaggle/working\"\n  outputs_dir: \"/kaggle/working/outputs\"\n  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n\"\"\"\n\n# Replace existing paths: block if present, else prepend it.\nif re.search(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", text):\n    text = re.sub(r\"(?m)^paths:\\n(?:^[ \\t].*\\n)+\", new_paths_block + \"\\n\", text)\nelse:\n    text = new_paths_block + \"\\n\" + text\n\ncfg_path.write_text(text, encoding=\"utf-8\")\nprint(\"✅ Patched paths block in base.yaml\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.438838Z","iopub.execute_input":"2026-02-27T05:58:09.439080Z","iopub.status.idle":"2026-02-27T05:58:09.448941Z","shell.execute_reply.started":"2026-02-27T05:58:09.439060Z","shell.execute_reply":"2026-02-27T05:58:09.448335Z"},"papermill":{"duration":0.011331,"end_time":"2026-02-25T05:46:27.593630","exception":false,"start_time":"2026-02-25T05:46:27.582299","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Patched paths block in base.yaml\n","output_type":"stream"}],"execution_count":13},{"id":"d245dd92","cell_type":"code","source":"!sed -n '1,120p' /kaggle/working/lesion-aware-dr/configs/base.yaml","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.449736Z","iopub.execute_input":"2026-02-27T05:58:09.449998Z","iopub.status.idle":"2026-02-27T05:58:09.572577Z","shell.execute_reply.started":"2026-02-27T05:58:09.449961Z","shell.execute_reply":"2026-02-27T05:58:09.571595Z"},"papermill":{"duration":0.131153,"end_time":"2026-02-25T05:46:27.728479","exception":false,"start_time":"2026-02-25T05:46:27.597326","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"project_name: \"lesion_aware_dr\"\nrun_name: \"efficientnet_cb_focal\"\n\npaths:\n  data_dir: \"/kaggle/working\"\n  raw_dir: \"/kaggle/working\"\n  processed_dir: \"/kaggle/working\"\n  outputs_dir: \"/kaggle/working/outputs\"\n  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n\n\n\n\n\nmodel:\n  backbone: \"efficientnet_b0\"\n  num_classes: 5\n  pretrained: true\n\ndata:\n  use_dummy: false\n  image_size: 224\n\n  train_csv: \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/train.csv\"\n  val_csv:   \"/content/drive/MyDrive/Lesion_aware_DR/eyepacs/val_clean.csv\"\n  image_dir: \"/content/eyepacs_images/Images\"\n  image_col: \"image_id\"\n  label_col: \"label\"\n\n  # Use ONE balancing method at a time.\n  # Here: sampler OFF, loss weights ON.\n  use_weighted_sampler: false","output_type":"stream"}],"execution_count":14},{"id":"2eb5e9be","cell_type":"code","source":"from pathlib import Path\n\nreq_path = Path(\"/kaggle/working/lesion-aware-dr/requirements_local.txt\")\n\n# try reading with utf-16 fallback\ntry:\n    text = req_path.read_text(encoding=\"utf-8\")\nexcept UnicodeDecodeError:\n    text = req_path.read_text(encoding=\"utf-16\")\n\nlines = text.splitlines()\n\nremove_keys = [\n    \"torch==\",\n    \"torchvision==\",\n    \"torchaudio==\",\n    \"opencv-python==\",\n]\n\ncleaned = [line for line in lines if not any(k in line for k in remove_keys)]\n\nneeded = [\"grad-cam\", \"torchmetrics\", \"einops\"]\n\nfor pkg in needed:\n    if not any(pkg in line for line in cleaned):\n        cleaned.append(pkg)\n\nreq_path.write_text(\"\\n\".join(cleaned), encoding=\"utf-8\")\n\nprint(\"✅ requirements fixed & converted to UTF-8\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.574123Z","iopub.execute_input":"2026-02-27T05:58:09.575024Z","iopub.status.idle":"2026-02-27T05:58:09.585169Z","shell.execute_reply.started":"2026-02-27T05:58:09.574983Z","shell.execute_reply":"2026-02-27T05:58:09.584444Z"},"papermill":{"duration":0.015559,"end_time":"2026-02-25T05:46:27.748095","exception":false,"start_time":"2026-02-25T05:46:27.732536","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ requirements fixed & converted to UTF-8\n","output_type":"stream"}],"execution_count":15},{"id":"fd2a95b9","cell_type":"code","source":"from pathlib import Path\n\nrepo = Path(\"/kaggle/working/lesion-aware-dr\")\n\ndata_dir = repo / \"src\" / \"data\"\ndata_dir.mkdir(parents=True, exist_ok=True)\n\n# make it a package\n(data_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n\ncode = r'''from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, Any\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n# ---------- Dummy dataset for quick testing ----------\n\nclass DummyDRDataset(Dataset):\n    \"\"\"\n    Returns random images & labels.\n    Use only when cfg.data.use_dummy = true.\n    \"\"\"\n    def __init__(self, num_samples: int, num_classes: int, image_size: int):\n        self.num_samples = int(num_samples)\n        self.num_classes = int(num_classes)\n        self.image_size = int(image_size)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        image = torch.rand(3, self.image_size, self.image_size)\n        label = torch.randint(0, self.num_classes, (1,)).item()\n        return {\"image\": image, \"label\": torch.tensor(label, dtype=torch.long)}\n\n\n# ---------- Real dataset for APTOS / EyePACS / etc. ----------\n\nclass DRDataset(Dataset):\n    \"\"\"\n    Generic DR dataset reading from CSV:\n      - image filename column (e.g., '16028_left.png')\n      - integer label column (0..num_classes-1)\n\n    Robust to corrupted images: retries a few times and then errors clearly.\n    \"\"\"\n\n    def __init__(\n        self,\n        csv_path: str,\n        images_dir: str,\n        image_col: str,\n        label_col: str,\n        image_size: int = 224,\n        augment: bool = False,\n        max_decode_retries: int = 20,\n        log_bad_every: int = 50,\n    ):\n        self.df = pd.read_csv(csv_path).reset_index(drop=True)\n        self.images_dir = Path(images_dir)\n        self.image_col = str(image_col)\n        self.label_col = str(label_col)\n        self.image_size = int(image_size)\n\n        self.max_decode_retries = int(max_decode_retries)\n        self.log_bad_every = int(log_bad_every)\n        self.bad_count = 0\n\n        # ✅ EfficientNet pretrained expects ImageNet normalization\n        imagenet_mean = (0.485, 0.456, 0.406)\n        imagenet_std = (0.229, 0.224, 0.225)\n\n        if augment:\n            self.transform = A.Compose(\n                [\n                    A.Resize(image_size, image_size),\n                    A.HorizontalFlip(p=0.5),\n                    A.RandomBrightnessContrast(p=0.4),\n                    # Replace ShiftScaleRotate with Affine (newer albumentations recommendation)\n                    A.Affine(\n                        scale=(0.95, 1.05),\n                        translate_percent=(0.0, 0.05),\n                        rotate=(-15, 15),\n                        p=0.5,\n                        mode=cv2.BORDER_REFLECT_101,\n                    ),\n                    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n                    ToTensorV2(),\n                ]\n            )\n        else:\n            self.transform = A.Compose(\n                [\n                    A.Resize(image_size, image_size),\n                    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n                    ToTensorV2(),\n                ]\n            )\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def _read_image(self, img_path: Path):\n        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n        if img is None:\n            return None\n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        for _ in range(self.max_decode_retries):\n            row = self.df.iloc[idx]\n            fname = str(row[self.image_col])\n            img_path = self.images_dir / fname\n\n            image = self._read_image(img_path)\n\n            if image is not None:\n                label = int(row[self.label_col])\n                augmented = self.transform(image=image)\n                return {\n                    \"image\": augmented[\"image\"],\n                    \"label\": torch.tensor(label, dtype=torch.long),\n                }\n\n            # Bad image encountered\n            self.bad_count += 1\n            if self.log_bad_every > 0 and self.bad_count % self.log_bad_every == 0:\n                print(f\"[WARN] Skipped {self.bad_count} corrupted images so far. Latest: {img_path}\")\n\n            idx = random.randint(0, len(self.df) - 1)\n\n        raise RuntimeError(\n            f\"Too many unreadable images encountered (>{self.max_decode_retries} retries). \"\n            f\"Check dataset integrity and image_dir. Last attempted: {img_path}\"\n        )\n\n\n# ---------- DataModule wrapper ----------\n\nclass DRDataModule:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.batch_size = int(cfg.training.batch_size)\n        self.num_workers = int(cfg.training.num_workers)\n        self.num_classes = int(cfg.model.num_classes)\n        self.image_size = int(cfg.data.image_size)\n\n        self.use_dummy = bool(cfg.data.get(\"use_dummy\", False))\n\n        # Optional sampler (only if you intentionally enable it in YAML)\n        self.use_weighted_sampler = bool(cfg.data.get(\"use_weighted_sampler\", False))\n        self.sampler_mode = str(cfg.data.get(\"sampler_mode\", \"inverse\")).lower()\n\n    def _pin_memory(self) -> bool:\n        return torch.cuda.is_available()\n\n    def _build_weighted_sampler_from_csv(self, csv_path: str, label_col: str) -> WeightedRandomSampler:\n        df = pd.read_csv(csv_path)\n        labels = df[label_col].astype(int).values\n\n        class_counts = np.bincount(labels, minlength=self.num_classes).astype(np.float32)\n        class_counts = np.maximum(class_counts, 1.0)\n\n        # inverse-frequency sampling\n        class_weights = 1.0 / class_counts\n        sample_weights = class_weights[labels]\n        sample_weights = torch.tensor(sample_weights, dtype=torch.double)\n\n        return WeightedRandomSampler(\n            weights=sample_weights,\n            num_samples=len(sample_weights),\n            replacement=True,\n        )\n\n    # ---- Dataloaders ----\n    def train_dataloader(self) -> DataLoader:\n        if self.use_dummy:\n            ds = DummyDRDataset(\n                num_samples=256,\n                num_classes=self.num_classes,\n                image_size=self.image_size,\n            )\n            return DataLoader(\n                ds,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=self.num_workers,\n                pin_memory=self._pin_memory(),\n            )\n\n        ds = DRDataset(\n            csv_path=self.cfg.data.train_csv,\n            images_dir=self.cfg.data.image_dir,\n            image_col=self.cfg.data.image_col,\n            label_col=self.cfg.data.label_col,\n            image_size=self.image_size,\n            augment=True,\n            max_decode_retries=30,\n            log_bad_every=50,\n        )\n\n        if self.use_weighted_sampler:\n            sampler = self._build_weighted_sampler_from_csv(\n                csv_path=self.cfg.data.train_csv,\n                label_col=self.cfg.data.label_col,\n            )\n            return DataLoader(\n                ds,\n                batch_size=self.batch_size,\n                shuffle=False,  # sampler + shuffle cannot both be True\n                sampler=sampler,\n                num_workers=self.num_workers,\n                pin_memory=self._pin_memory(),\n            )\n\n        return DataLoader(\n            ds,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=self._pin_memory(),\n        )\n\n    def val_dataloader(self) -> DataLoader:\n        if self.use_dummy:\n            ds = DummyDRDataset(\n                num_samples=64,\n                num_classes=self.num_classes,\n                image_size=self.image_size,\n            )\n        else:\n            ds = DRDataset(\n                csv_path=self.cfg.data.val_csv,\n                images_dir=self.cfg.data.image_dir,\n                image_col=self.cfg.data.image_col,\n                label_col=self.cfg.data.label_col,\n                image_size=self.image_size,\n                augment=False,\n                max_decode_retries=30,\n                log_bad_every=50,\n            )\n\n        return DataLoader(\n            ds,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=self._pin_memory(),\n        )\n'''\n\n(data_dir / \"dr_datamodule.py\").write_text(code, encoding=\"utf-8\")\n\nprint(\"✅ Created:\", data_dir / \"dr_datamodule.py\")","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.586809Z","iopub.execute_input":"2026-02-27T05:58:09.587067Z","iopub.status.idle":"2026-02-27T05:58:09.600661Z","shell.execute_reply.started":"2026-02-27T05:58:09.587045Z","shell.execute_reply":"2026-02-27T05:58:09.599843Z"},"papermill":{"duration":0.015655,"end_time":"2026-02-25T05:46:27.767651","exception":false,"start_time":"2026-02-25T05:46:27.751996","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Created: /kaggle/working/lesion-aware-dr/src/data/dr_datamodule.py\n","output_type":"stream"}],"execution_count":16},{"id":"0492d115","cell_type":"code","source":"from pathlib import Path\np = Path(\"/kaggle/working/lesion-aware-dr/requirements_local.txt\")\nprint(p.read_text()[:200])","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.601680Z","iopub.execute_input":"2026-02-27T05:58:09.601945Z","iopub.status.idle":"2026-02-27T05:58:09.615228Z","shell.execute_reply.started":"2026-02-27T05:58:09.601923Z","shell.execute_reply":"2026-02-27T05:58:09.614691Z"},"papermill":{"duration":0.009966,"end_time":"2026-02-25T05:46:27.781568","exception":false,"start_time":"2026-02-25T05:46:27.771602","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"albucore==0.0.24\nalbumentations==2.0.8\naltair==5.5.0\nannotated-types==0.7.0\nantlr4-python3-runtime==4.9.3\nanyio==4.12.0\nattrs==25.4.0\nblinker==1.9.0\ncachetools==6.2.2\ncertifi==2025.11.12\ncharset-norma\n","output_type":"stream"}],"execution_count":17},{"id":"eca93054","cell_type":"code","source":"!pip install -q -r /kaggle/working/lesion-aware-dr/requirements_local.txt","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:58:09.616033Z","iopub.execute_input":"2026-02-27T05:58:09.616302Z","iopub.status.idle":"2026-02-27T05:59:26.997707Z","shell.execute_reply.started":"2026-02-27T05:58:09.616268Z","shell.execute_reply":"2026-02-27T05:59:26.996749Z"},"papermill":{"duration":75.871842,"end_time":"2026-02-25T05:47:43.657367","exception":false,"start_time":"2026-02-25T05:46:27.785525","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.1/516.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.0/425.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.3/395.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.1/653.1 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.2/231.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\ntransformers 5.2.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 1.1.6 which is incompatible.\nydata-profiling 4.18.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.5.1, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\ndatasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ngradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":18},{"id":"b745d95d","cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2026-02-27T05:59:26.999107Z","iopub.execute_input":"2026-02-27T05:59:26.999444Z","iopub.status.idle":"2026-02-27T05:59:31.200505Z","shell.execute_reply.started":"2026-02-27T05:59:26.999416Z","shell.execute_reply":"2026-02-27T05:59:31.199797Z"},"papermill":{"duration":4.016089,"end_time":"2026-02-25T05:47:47.679717","exception":false,"start_time":"2026-02-25T05:47:43.663628","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":19},{"id":"96e5d0ae","cell_type":"code","source":"from pathlib import Path\n\ncfg_path = Path(\"/kaggle/working/lesion-aware-dr/configs/base.yaml\")\n\nclean_yaml = \"\"\"project_name: \"lesion_aware_dr\"\nrun_name: \"efficientnet_focal_g15_cw\"\n\npaths:\n  data_dir: \"/kaggle/working\"\n  raw_dir: \"/kaggle/working\"\n  processed_dir: \"/kaggle/working\"\n  outputs_dir: \"/kaggle/working/outputs\"\n  checkpoints_dir: \"/kaggle/working/outputs/checkpoints\"\n\nmodel:\n  backbone: \"efficientnet_b0\"\n  num_classes: 5\n  pretrained: true\n\ndata:\n  use_dummy: false\n  image_size: 224\n\n  train_csv: \"/kaggle/working/train.csv\"\n  val_csv:   \"/kaggle/working/val.csv\"\n  image_dir: \"/kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\"\n  image_col: \"image_id\"\n  label_col: \"label\"\n\n  use_weighted_sampler: false\n  sampler_mode: \"inverse\"\n\ntraining:\n  seed: 42\n  epochs: 12\n  batch_size: 16\n  num_workers: 2\n  lr: 1e-4\n  weight_decay: 1e-5\n  optimizer: \"adamw\"\n  scheduler: \"cosine\"\n\nlogging:\n  use_wandb: false\n  wandb_project: \"lesion_aware_dr\"\n  wandb_entity: null\n\nloss:\n  name: \"cb_focal\"\n  beta: 0.999\n  gamma: 2.0\n  alpha_mode: \"effective\"\n  reduction: \"mean\"\n  use_class_weights: false\n\"\"\"\n\ncfg_path.write_text(clean_yaml, encoding=\"utf-8\")\nprint(\"✅ Rewrote base.yaml cleanly:\", cfg_path)","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:12:24.885924Z","iopub.execute_input":"2026-02-27T06:12:24.886228Z","iopub.status.idle":"2026-02-27T06:12:24.891939Z","shell.execute_reply.started":"2026-02-27T06:12:24.886200Z","shell.execute_reply":"2026-02-27T06:12:24.891353Z"},"papermill":{"duration":0.014209,"end_time":"2026-02-25T05:47:47.700540","exception":false,"start_time":"2026-02-25T05:47:47.686331","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"✅ Rewrote base.yaml cleanly: /kaggle/working/lesion-aware-dr/configs/base.yaml\n","output_type":"stream"}],"execution_count":23},{"id":"58d61603","cell_type":"code","source":"%cd /kaggle/working/lesion-aware-dr\n!python -m src.train --cfg_path configs/base.yaml","metadata":{"execution":{"iopub.status.busy":"2026-02-27T06:12:29.794492Z","iopub.execute_input":"2026-02-27T06:12:29.795094Z","iopub.status.idle":"2026-02-27T08:11:53.856543Z","shell.execute_reply.started":"2026-02-27T06:12:29.795064Z","shell.execute_reply":"2026-02-27T08:11:53.855762Z"},"papermill":{"duration":6830.364683,"end_time":"2026-02-25T07:41:38.071329","exception":false,"start_time":"2026-02-25T05:47:47.706646","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/lesion-aware-dr\n[2026-02-27 06:12:38] [INFO] Starting training script\n[2026-02-27 06:12:38] [INFO] Using device: cuda\n[2026-02-27 06:12:38] [INFO] [RUN DIR] New run folder: /kaggle/working/outputs/2026-02-27\n[2026-02-27 06:12:38] [INFO] [RUN DIR] Checkpoints folder: /kaggle/working/outputs/2026-02-27/checkpoints\n[2026-02-27 06:12:38] [INFO] Config:\nproject_name: lesion_aware_dr\nrun_name: efficientnet_focal_g15_cw\npaths:\n  data_dir: /kaggle/working\n  raw_dir: /kaggle/working\n  processed_dir: /kaggle/working\n  outputs_dir: /kaggle/working/outputs/2026-02-27\n  checkpoints_dir: /kaggle/working/outputs/2026-02-27/checkpoints\nmodel:\n  backbone: efficientnet_b0\n  num_classes: 5\n  pretrained: true\ndata:\n  use_dummy: false\n  image_size: 224\n  train_csv: /kaggle/working/train.csv\n  val_csv: /kaggle/working/val.csv\n  image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\n  image_col: image_id\n  label_col: label\n  use_weighted_sampler: false\n  sampler_mode: inverse\ntraining:\n  seed: 42\n  epochs: 12\n  batch_size: 16\n  num_workers: 2\n  lr: 0.0001\n  weight_decay: 1.0e-05\n  optimizer: adamw\n  scheduler: cosine\nlogging:\n  use_wandb: false\n  wandb_project: lesion_aware_dr\n  wandb_entity: null\nloss:\n  name: cb_focal\n  beta: 0.999\n  gamma: 2.0\n  alpha_mode: effective\n  reduction: mean\n  use_class_weights: false\n\n/kaggle/working/lesion-aware-dr/src/data/dr_datamodule.py:81: UserWarning: Argument(s) 'mode' are not valid for transform Affine\n  A.Affine(\n[2026-02-27 06:12:38] [INFO] Train samples: 70960\n[2026-02-27 06:12:38] [INFO] Val samples:   17740\n[2026-02-27 06:12:38] [INFO] Train batches: 4435\n[2026-02-27 06:12:38] [INFO] Val batches:   1109\nmodel.safetensors: 100%|███████████████████| 21.4M/21.4M [00:00<00:00, 34.3MB/s]\n/kaggle/working/lesion-aware-dr/src/train.py:251: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\nEpoch 1/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 1/12 [TRAIN]:  13%|▏| 594/4435 [01:43<10:13,  6.26it/s, loss=0.1169, lr=1.libpng error: Read Error\nEpoch 1/12 [TRAIN]: 100%|█| 4435/4435 [12:17<00:00,  6.02it/s, loss=0.3957, lr=1\nEpoch 1/12 [VAL]: 100%|████| 1109/1109 [03:12<00:00,  5.75it/s, val_loss=0.1200]\n[2026-02-27 06:28:10] [INFO] Epoch [1/12] train_loss=0.4272 val_loss=0.2921 val_acc=0.7848 val_f1=0.4192 (best_f1=-1.0000, best_loss=inf)\n[2026-02-27 06:28:11] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4192)\n[2026-02-27 06:28:11] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2921)\nEpoch 2/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 2/12 [TRAIN]:  93%|▉| 4136/4435 [07:20<00:31,  9.50it/s, loss=0.2308, lr=9libpng error: Read Error\nEpoch 2/12 [TRAIN]: 100%|█| 4435/4435 [07:51<00:00,  9.41it/s, loss=0.2475, lr=9\nEpoch 2/12 [VAL]: 100%|████| 1109/1109 [01:38<00:00, 11.20it/s, val_loss=0.1351]\n[2026-02-27 06:37:41] [INFO] Epoch [2/12] train_loss=0.2845 val_loss=0.2671 val_acc=0.7961 val_f1=0.4767 (best_f1=0.4192, best_loss=0.2921)\n[2026-02-27 06:37:42] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4767)\n[2026-02-27 06:37:42] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2671)\nEpoch 3/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 3/12 [TRAIN]:   3%| | 147/4435 [00:15<07:06, 10.06it/s, loss=0.3150, lr=9.libpng error: Read Error\nEpoch 3/12 [TRAIN]: 100%|█| 4435/4435 [07:46<00:00,  9.50it/s, loss=0.1345, lr=9\nEpoch 3/12 [VAL]: 100%|████| 1109/1109 [01:40<00:00, 11.00it/s, val_loss=0.1800]\n[2026-02-27 06:47:09] [INFO] Epoch [3/12] train_loss=0.2594 val_loss=0.2575 val_acc=0.7999 val_f1=0.4822 (best_f1=0.4767, best_loss=0.2671)\n[2026-02-27 06:47:10] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4822)\n[2026-02-27 06:47:10] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2575)\nEpoch 4/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 4/12 [TRAIN]:  63%|▋| 2782/4435 [04:55<02:59,  9.23it/s, loss=0.2685, lr=8libpng error: Read Error\nEpoch 4/12 [TRAIN]: 100%|█| 4435/4435 [07:52<00:00,  9.39it/s, loss=0.2557, lr=8\nEpoch 4/12 [VAL]: 100%|████| 1109/1109 [01:42<00:00, 10.85it/s, val_loss=0.1692]\n[2026-02-27 06:56:45] [INFO] Epoch [4/12] train_loss=0.2400 val_loss=0.2455 val_acc=0.8094 val_f1=0.4961 (best_f1=0.4822, best_loss=0.2575)\n[2026-02-27 06:56:45] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4961)\n[2026-02-27 06:56:45] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2455)\nEpoch 5/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 5/12 [TRAIN]:  90%|▉| 4000/4435 [06:54<00:47,  9.23it/s, loss=0.1282, lr=7libpng error: Read Error\nEpoch 5/12 [TRAIN]: 100%|█| 4435/4435 [07:39<00:00,  9.66it/s, loss=0.1497, lr=7\nEpoch 5/12 [VAL]: 100%|████| 1109/1109 [01:35<00:00, 11.58it/s, val_loss=0.1777]\n[2026-02-27 07:06:00] [INFO] Epoch [5/12] train_loss=0.2227 val_loss=0.2414 val_acc=0.8188 val_f1=0.4991 (best_f1=0.4961, best_loss=0.2455)\n[2026-02-27 07:06:00] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.4991)\n[2026-02-27 07:06:01] [INFO] ✅ Saved best val-loss model for THIS RUN (val_loss=0.2414)\nEpoch 6/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 6/12 [TRAIN]:  40%|▍| 1754/4435 [03:05<04:38,  9.64it/s, loss=0.2403, lr=6libpng error: Read Error\nEpoch 6/12 [TRAIN]: 100%|█| 4435/4435 [07:36<00:00,  9.71it/s, loss=0.1505, lr=6\nEpoch 6/12 [VAL]: 100%|████| 1109/1109 [01:35<00:00, 11.60it/s, val_loss=0.1814]\n[2026-02-27 07:15:13] [INFO] Epoch [6/12] train_loss=0.2062 val_loss=0.2563 val_acc=0.8080 val_f1=0.4482 (best_f1=0.4991, best_loss=0.2414)\nEpoch 7/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 7/12 [TRAIN]:  41%|▍| 1824/4435 [03:03<04:34,  9.51it/s, loss=0.1985, lr=5libpng error: Read Error\nEpoch 7/12 [TRAIN]: 100%|█| 4435/4435 [07:34<00:00,  9.76it/s, loss=0.0801, lr=5\nEpoch 7/12 [VAL]: 100%|████| 1109/1109 [01:37<00:00, 11.35it/s, val_loss=0.1399]\n[2026-02-27 07:24:25] [INFO] Epoch [7/12] train_loss=0.1880 val_loss=0.2554 val_acc=0.8086 val_f1=0.5000 (best_f1=0.4991, best_loss=0.2414)\n[2026-02-27 07:24:26] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5000)\nEpoch 8/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 8/12 [TRAIN]:   7%| | 305/4435 [00:31<07:08,  9.65it/s, loss=0.1505, lr=3.libpng error: Read Error\nEpoch 8/12 [TRAIN]: 100%|█| 4435/4435 [07:50<00:00,  9.43it/s, loss=0.1137, lr=3\nEpoch 8/12 [VAL]: 100%|████| 1109/1109 [01:36<00:00, 11.50it/s, val_loss=0.1387]\n[2026-02-27 07:33:52] [INFO] Epoch [8/12] train_loss=0.1684 val_loss=0.2636 val_acc=0.8043 val_f1=0.5152 (best_f1=0.5000, best_loss=0.2414)\n[2026-02-27 07:33:53] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5152)\nEpoch 9/12 [TRAIN]:   0%|                              | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 9/12 [TRAIN]:  42%|▍| 1851/4435 [03:14<04:36,  9.33it/s, loss=0.0962, lr=2libpng error: Read Error\nEpoch 9/12 [TRAIN]: 100%|█| 4435/4435 [07:48<00:00,  9.46it/s, loss=0.1847, lr=2\nEpoch 9/12 [VAL]: 100%|████| 1109/1109 [01:35<00:00, 11.62it/s, val_loss=0.1383]\n[2026-02-27 07:43:17] [INFO] Epoch [9/12] train_loss=0.1518 val_loss=0.2800 val_acc=0.8053 val_f1=0.5210 (best_f1=0.5152, best_loss=0.2414)\n[2026-02-27 07:43:17] [INFO] ✅ Saved best macro-F1 model for THIS RUN (val_f1=0.5210)\nEpoch 10/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 10/12 [TRAIN]:  60%|▌| 2659/4435 [04:40<03:11,  9.26it/s, loss=0.1877, lr=libpng error: Read Error\nEpoch 10/12 [TRAIN]: 100%|█| 4435/4435 [07:55<00:00,  9.33it/s, loss=0.0676, lr=\nEpoch 10/12 [VAL]: 100%|███| 1109/1109 [01:36<00:00, 11.47it/s, val_loss=0.1229]\n[2026-02-27 07:52:49] [INFO] Epoch [10/12] train_loss=0.1380 val_loss=0.2881 val_acc=0.8063 val_f1=0.5067 (best_f1=0.5210, best_loss=0.2414)\nEpoch 11/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nlibpng error: Read Error\nEpoch 11/12 [TRAIN]: 100%|█| 4435/4435 [08:01<00:00,  9.22it/s, loss=0.0709, lr=\nEpoch 11/12 [VAL]: 100%|███| 1109/1109 [01:38<00:00, 11.26it/s, val_loss=0.1223]\n[2026-02-27 08:02:29] [INFO] Epoch [11/12] train_loss=0.1286 val_loss=0.2973 val_acc=0.7999 val_f1=0.5086 (best_f1=0.5210, best_loss=0.2414)\nEpoch 12/12 [TRAIN]:   0%|                             | 0/4435 [00:00<?, ?it/s]/kaggle/working/lesion-aware-dr/src/train.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 12/12 [TRAIN]:  43%|▍| 1893/4435 [03:16<04:25,  9.56it/s, loss=0.1800, lr=libpng error: Read Error\nEpoch 12/12 [TRAIN]: 100%|█| 4435/4435 [07:43<00:00,  9.58it/s, loss=0.0510, lr=\nEpoch 12/12 [VAL]: 100%|███| 1109/1109 [01:38<00:00, 11.28it/s, val_loss=0.1219]\n[2026-02-27 08:11:50] [INFO] Epoch [12/12] train_loss=0.1237 val_loss=0.2966 val_acc=0.7960 val_f1=0.5128 (best_f1=0.5210, best_loss=0.2414)\n[2026-02-27 08:11:51] [INFO] Training completed. Run saved at: /kaggle/working/outputs/2026-02-27\n","output_type":"stream"}],"execution_count":24},{"id":"befbce7a-6860-4549-a048-346f33426e49","cell_type":"code","source":"from pathlib import Path\n\ncfg_path = Path(\"/kaggle/working/lesion-aware-dr/src/gradcam_eval.py\")\n\nclean_py = r\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport yaml\nimport numpy as np\nimport cv2\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom pytorch_grad_cam import GradCAM, GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n\nfrom src.models import build_model\n\n\n# -----------------------------\n# Dataset (same CSV format as eval)\n# -----------------------------\nclass EyePacsDataset(Dataset):\n    def __init__(\n        self,\n        csv_path: str,\n        image_dir: str,\n        image_col: str,\n        label_col: str,\n        transform=None,\n    ):\n        import pandas as pd\n\n        self.df = pd.read_csv(csv_path)\n        self.image_dir = str(image_dir)\n        self.image_col = str(image_col)\n        self.label_col = str(label_col)\n        self.transform = transform\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        row = self.df.iloc[idx]\n        fname = str(row[self.image_col])\n        label = int(row[self.label_col])\n\n        img_path = str(Path(self.image_dir) / fname)\n        bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        if bgr is None:\n            raise FileNotFoundError(f\"Could not read image: {img_path}\")\n\n        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            x = self.transform(image=rgb)[\"image\"]\n        else:\n            x = torch.from_numpy(rgb).permute(2, 0, 1).float() / 255.0\n\n        return {\n            \"image\": x,\n            \"label\": torch.tensor(label, dtype=torch.long),\n            \"fname\": fname,\n            \"rgb\": rgb,  # uint8 RGB for overlay\n        }\n\n\ndef load_cfg(path: str) -> Dict[str, Any]:\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef build_transform(image_size: int) -> A.Compose:\n    # ImageNet normalization (matches EfficientNet pretrained)\n    return A.Compose(\n        [\n            A.Resize(image_size, image_size),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n\n\ndef load_checkpoint(model: torch.nn.Module, ckpt_path: str, device: torch.device) -> None:\n    ckpt = torch.load(ckpt_path, map_location=device)\n\n    # Your train saves {\"model_state\": ...}\n    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n        state = ckpt[\"model_state\"]\n    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n        state = ckpt[\"state_dict\"]\n    elif isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n        state = ckpt[\"model_state_dict\"]\n    else:\n        state = ckpt\n\n    state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n    model.load_state_dict(state, strict=True)\n\n\ndef find_last_conv_layer(model: torch.nn.Module) -> torch.nn.Module:\n    last_conv = None\n    for m in model.modules():\n        if isinstance(m, torch.nn.Conv2d):\n            last_conv = m\n    if last_conv is None:\n        raise RuntimeError(\"No Conv2d found. Can't run Grad-CAM.\")\n    return last_conv\n\n\ndef overlay_cam(rgb_uint8: np.ndarray, cam_2d: np.ndarray, alpha: float = 0.45) -> np.ndarray:\n    h, w = rgb_uint8.shape[:2]\n    cam_resized = cv2.resize(cam_2d, (w, h))\n    cam_uint8 = np.uint8(255 * np.clip(cam_resized, 0, 1))\n\n    heatmap_bgr = cv2.applyColorMap(cam_uint8, cv2.COLORMAP_JET)\n    heatmap_rgb = cv2.cvtColor(heatmap_bgr, cv2.COLOR_BGR2RGB)\n\n    out = (1 - alpha) * rgb_uint8.astype(np.float32) + alpha * heatmap_rgb.astype(np.float32)\n    return np.clip(out, 0, 255).astype(np.uint8)\n\n\ndef parse_args():\n    ap = argparse.ArgumentParser()\n\n    # Keep BOTH styles: --cfg_path/--ckpt_path and --cfg/--ckpt\n    ap.add_argument(\"--cfg_path\", type=str, default=None)\n    ap.add_argument(\"--ckpt_path\", type=str, default=None)\n    ap.add_argument(\"--cfg\", type=str, default=None)\n    ap.add_argument(\"--ckpt\", type=str, default=None)\n\n    ap.add_argument(\"--split\", type=str, default=\"val\", choices=[\"val\", \"train\"])\n    ap.add_argument(\"--method\", type=str, default=\"gradcampp\", choices=[\"gradcam\", \"gradcampp\"])\n    ap.add_argument(\"--target\", type=str, default=\"pred\", choices=[\"pred\", \"true\"])\n\n    ap.add_argument(\"--num_images\", type=int, default=30)\n    ap.add_argument(\"--batch_size\", type=int, default=16)\n    ap.add_argument(\"--num_workers\", type=int, default=0)\n    ap.add_argument(\"--seed\", type=int, default=42)\n    ap.add_argument(\"--alpha\", type=float, default=0.45)\n\n    return ap.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    cfg_path = args.cfg_path or args.cfg\n    ckpt_path = args.ckpt_path or args.ckpt\n\n    if not cfg_path or not ckpt_path:\n        raise ValueError(\"Provide config + checkpoint: use --cfg_path/--ckpt_path OR --cfg/--ckpt\")\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n\n    cfg = load_cfg(cfg_path)\n\n    backbone = cfg[\"model\"][\"backbone\"]\n    num_classes = int(cfg[\"model\"][\"num_classes\"])\n\n    image_size = int(cfg[\"data\"][\"image_size\"])\n    image_dir = cfg[\"data\"][\"image_dir\"]\n    image_col = cfg[\"data\"][\"image_col\"]\n    label_col = cfg[\"data\"][\"label_col\"]\n\n    csv_path = cfg[\"data\"][\"val_csv\"] if args.split == \"val\" else cfg[\"data\"][\"train_csv\"]\n\n    # IMPORTANT: your training now changes cfg.paths.outputs_dir to a RUN folder.\n    # So Grad-CAM outputs will go into THAT run folder automatically.\n    outputs_dir = Path(cfg[\"paths\"][\"outputs_dir\"])\n    out_dir = outputs_dir / \"gradcam\" / f\"{args.method}_{args.split}_{args.target}\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"[GradCAM] device: {device}\")\n    print(f\"[GradCAM] csv: {csv_path}\")\n    print(f\"[GradCAM] image_dir: {image_dir}\")\n    print(f\"[GradCAM] ckpt: {ckpt_path}\")\n    print(f\"[GradCAM] out_dir: {out_dir}\")\n\n    tfm = build_transform(image_size)\n    ds = EyePacsDataset(csv_path, image_dir, image_col, label_col, transform=tfm)\n    dl = DataLoader(\n        ds,\n        batch_size=args.batch_size,\n        shuffle=False,\n        num_workers=args.num_workers,\n        pin_memory=(device.type == \"cuda\"),\n    )\n\n    # Same model builder as training\n    model = build_model(backbone=backbone, num_classes=num_classes, pretrained=False).to(device)\n    load_checkpoint(model, ckpt_path, device)\n    model.eval()\n\n    target_layer = find_last_conv_layer(model)\n    print(f\"[GradCAM] Using target layer: {target_layer}\")\n\n    cam_class = GradCAMPlusPlus if args.method == \"gradcampp\" else GradCAM\n    cam = cam_class(model=model, target_layers=[target_layer])\n\n    softmax = torch.nn.Softmax(dim=1)\n\n    exported = 0\n    for batch in dl:\n        if exported >= args.num_images:\n            break\n\n        xb = batch[\"image\"].to(device, non_blocking=True)\n        yb = batch[\"label\"].cpu().numpy()\n        names = batch[\"fname\"]\n        rgbs = batch[\"rgb\"].numpy()\n\n        with torch.no_grad():\n            logits = model(xb)\n            probs = softmax(logits)\n            pred = torch.argmax(probs, dim=1).detach().cpu().numpy()\n\n        target_classes = pred if args.target == \"pred\" else yb\n        targets = [ClassifierOutputTarget(int(c)) for c in target_classes]\n\n        # Grad-CAM needs gradients => no no_grad here\n        cams = cam(input_tensor=xb, targets=targets)  # (B, H, W)\n\n        for i in range(len(names)):\n            if exported >= args.num_images:\n                break\n\n            rgb = rgbs[i]\n            if rgb.dtype != np.uint8:\n                rgb = np.clip(rgb, 0, 255).astype(np.uint8)\n\n            overlay = overlay_cam(rgb, cams[i], alpha=float(args.alpha))\n\n            out_name = (\n                f\"{exported:04d}__true{yb[i]}__pred{pred[i]}__target{target_classes[i]}__{Path(names[i]).stem}.png\"\n            )\n            out_path = out_dir / out_name\n\n            cv2.imwrite(str(out_path), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n            exported += 1\n\n    print(f\"[GradCAM] Done. Exported {exported} images to: {out_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\ncfg_path.write_text(clean_py, encoding=\"utf-8\")\nprint(\"✅ Wrote:\", cfg_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:11:53.858357Z","iopub.execute_input":"2026-02-27T08:11:53.858667Z","iopub.status.idle":"2026-02-27T08:11:53.868515Z","shell.execute_reply.started":"2026-02-27T08:11:53.858641Z","shell.execute_reply":"2026-02-27T08:11:53.867744Z"}},"outputs":[{"name":"stdout","text":"✅ Wrote: /kaggle/working/lesion-aware-dr/src/gradcam_eval.py\n","output_type":"stream"}],"execution_count":25},{"id":"5d38167c-9993-4fba-a93e-bc8ec5677e2e","cell_type":"code","source":"!ls -lh /kaggle/working/outputs/2026-02-27/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:55:16.694147Z","iopub.execute_input":"2026-02-27T08:55:16.694757Z","iopub.status.idle":"2026-02-27T08:55:16.827894Z","shell.execute_reply.started":"2026-02-27T08:55:16.694725Z","shell.execute_reply":"2026-02-27T08:55:16.827155Z"}},"outputs":[{"name":"stdout","text":"total 140M\n-rw-r--r-- 1 root root 47M Feb 27 07:43 best_macro_f1_model.pt\n-rw-r--r-- 1 root root 47M Feb 27 07:06 best_val_loss_model.pt\n-rw-r--r-- 1 root root 47M Feb 27 08:11 last_model.pt\n","output_type":"stream"}],"execution_count":33},{"id":"20cc1cba-7aba-4527-bc92-bf918eb99956","cell_type":"code","source":"!python -m src.eval \\\n--cfg configs/base.yaml \\\n--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:58:41.311161Z","iopub.execute_input":"2026-02-27T08:58:41.311392Z","iopub.status.idle":"2026-02-27T09:00:11.666935Z","shell.execute_reply.started":"2026-02-27T08:58:41.311368Z","shell.execute_reply":"2026-02-27T09:00:11.665804Z"}},"outputs":[{"name":"stdout","text":"[EVAL] device: cuda\n[EVAL] csv: /kaggle/working/val.csv\n[EVAL] image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\n[EVAL] ckpt: /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt\n\n[EVAL RESULTS]\nAccuracy : 0.8057\nMacro F1 : 0.5211\n\nClassification report:\n\n              precision    recall  f1-score   support\n\n           0     0.8620    0.9454    0.9018     13069\n           1     0.2563    0.0572    0.0935      1241\n           2     0.6080    0.5703    0.5886      2630\n           3     0.4253    0.3957    0.4099       417\n           4     0.7336    0.5248    0.6119       383\n\n    accuracy                         0.8057     17740\n   macro avg     0.5770    0.4987    0.5211     17740\nweighted avg     0.7689    0.8057    0.7810     17740\n\n\nSaved to: /kaggle/working/outputs/eval\n","output_type":"stream"}],"execution_count":37},{"id":"fe2b68df-ff49-4911-8d78-b81fab97c331","cell_type":"code","source":"!python -m src.eval \\\n--cfg configs/base.yaml \\\n--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_val_loss_model.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T09:01:42.545456Z","iopub.execute_input":"2026-02-27T09:01:42.546123Z","iopub.status.idle":"2026-02-27T09:03:12.470457Z","shell.execute_reply.started":"2026-02-27T09:01:42.546095Z","shell.execute_reply":"2026-02-27T09:03:12.469484Z"}},"outputs":[{"name":"stdout","text":"[EVAL] device: cuda\n[EVAL] csv: /kaggle/working/val.csv\n[EVAL] image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\n[EVAL] ckpt: /kaggle/working/outputs/2026-02-27/checkpoints/best_val_loss_model.pt\n\n[EVAL RESULTS]\nAccuracy : 0.8191\nMacro F1 : 0.4993\n\nClassification report:\n\n              precision    recall  f1-score   support\n\n           0     0.8429    0.9800    0.9063     13069\n           1     0.8333    0.0040    0.0080      1241\n           2     0.6737    0.5361    0.5971      2630\n           3     0.5415    0.2662    0.3569       417\n           4     0.8133    0.5117    0.6282       383\n\n    accuracy                         0.8191     17740\n   macro avg     0.7409    0.4596    0.4993     17740\nweighted avg     0.8094    0.8191    0.7787     17740\n\n\nSaved to: /kaggle/working/outputs/eval\n","output_type":"stream"}],"execution_count":39},{"id":"9cf9cfb6-575c-42e4-a2c7-029212b4c564","cell_type":"code","source":"!python -m src.eval \\\n--cfg configs/base.yaml \\\n--ckpt /kaggle/working/outputs/2026-02-27/checkpoints/last_model.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:57:04.015050Z","iopub.execute_input":"2026-02-27T08:57:04.015764Z","iopub.status.idle":"2026-02-27T08:58:41.309464Z","shell.execute_reply.started":"2026-02-27T08:57:04.015735Z","shell.execute_reply":"2026-02-27T08:58:41.308612Z"}},"outputs":[{"name":"stdout","text":"[EVAL] device: cuda\n[EVAL] csv: /kaggle/working/val.csv\n[EVAL] image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\n[EVAL] ckpt: /kaggle/working/outputs/2026-02-27/checkpoints/last_model.pt\n\n[EVAL RESULTS]\nAccuracy : 0.7967\nMacro F1 : 0.5131\n\nClassification report:\n\n              precision    recall  f1-score   support\n\n           0     0.8603    0.9370    0.8970     13069\n           1     0.2005    0.0717    0.1056      1241\n           2     0.6021    0.5449    0.5721      2630\n           3     0.4269    0.3501    0.3847       417\n           4     0.6441    0.5718    0.6058       383\n\n    accuracy                         0.7967     17740\n   macro avg     0.5468    0.4951    0.5131     17740\nweighted avg     0.7610    0.7967    0.7752     17740\n\n\nSaved to: /kaggle/working/outputs/eval\n","output_type":"stream"}],"execution_count":36},{"id":"3e197a6e-c602-47c7-9909-627e40837b6b","cell_type":"code","source":"!python -m src.gradcam_eval \\\n  --cfg configs/base.yaml \\\n  --ckpt /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt \\\n  --split val \\\n  --method gradcampp \\\n  --target pred \\\n  --num_images 30 \\\n  --batch_size 16 \\\n  --num_workers 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:12:27.580120Z","iopub.execute_input":"2026-02-27T08:12:27.580392Z","iopub.status.idle":"2026-02-27T08:12:38.419032Z","shell.execute_reply.started":"2026-02-27T08:12:27.580366Z","shell.execute_reply":"2026-02-27T08:12:38.418260Z"}},"outputs":[{"name":"stdout","text":"[GradCAM] device: cuda\n[GradCAM] csv: /kaggle/working/val.csv\n[GradCAM] image_dir: /kaggle/input/datasets/mohlamin/resized-eyepacs-diabetic-retinopathy-dataset/Images\n[GradCAM] ckpt: /kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt\n[GradCAM] out_dir: /kaggle/working/outputs/gradcam/gradcampp_val_pred\n[GradCAM] Using target layer: Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n[GradCAM] Done. Exported 30 images to: /kaggle/working/outputs/gradcam/gradcampp_val_pred\n","output_type":"stream"}],"execution_count":32},{"id":"735c114c-966e-4090-8e53-927b88affd12","cell_type":"code","source":"!zip -r dr_models.zip /kaggle/working/outputs/2026-02-27/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T08:55:59.797232Z","iopub.execute_input":"2026-02-27T08:55:59.797586Z","iopub.status.idle":"2026-02-27T08:56:07.154929Z","shell.execute_reply.started":"2026-02-27T08:55:59.797550Z","shell.execute_reply":"2026-02-27T08:56:07.154167Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/outputs/2026-02-27/checkpoints/ (stored 0%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt (deflated 8%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/best_val_loss_model.pt (deflated 8%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/last_model.pt (deflated 8%)\n","output_type":"stream"}],"execution_count":34},{"id":"cd4ffa1a-8855-4f2e-a344-281984a0d300","cell_type":"code","source":"!zip -r models.zip /kaggle/working/outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T09:03:29.675163Z","iopub.execute_input":"2026-02-27T09:03:29.675516Z","iopub.status.idle":"2026-02-27T09:03:37.443773Z","shell.execute_reply.started":"2026-02-27T09:03:29.675487Z","shell.execute_reply":"2026-02-27T09:03:37.443002Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/outputs/ (stored 0%)\n  adding: kaggle/working/outputs/2026-02-27/ (stored 0%)\n  adding: kaggle/working/outputs/2026-02-27/config_used.yaml (deflated 52%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/ (stored 0%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/best_macro_f1_model.pt (deflated 8%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/best_val_loss_model.pt (deflated 8%)\n  adding: kaggle/working/outputs/2026-02-27/checkpoints/last_model.pt (deflated 8%)\n  adding: kaggle/working/outputs/.ipynb_checkpoints/ (stored 0%)\n  adding: kaggle/working/outputs/gradcam/ (stored 0%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/ (stored 0%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0009__true1__pred1__target1__36573_right.png (deflated 3%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0002__true0__pred0__target0__34118_right.png (deflated 6%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0029__true0__pred0__target0__31349_left.png (deflated 2%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0010__true0__pred0__target0__29643_right.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0013__true0__pred0__target0__1361_left.png (deflated 3%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0019__true0__pred2__target2__41359_left.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0020__true0__pred0__target0__24239_right.png (deflated 17%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0004__true0__pred1__target1__24143_left.png (deflated 7%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0003__true0__pred0__target0__34135_left.png (deflated 2%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0018__true0__pred0__target0__14538_left.png (deflated 15%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0001__true0__pred0__target0__6529_left.png (deflated 4%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0017__true0__pred0__target0__24352_left.png (deflated 5%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0007__true0__pred0__target0__34059_right.png (deflated 3%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0022__true0__pred0__target0__11832_left.png (deflated 4%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0005__true0__pred0__target0__25255_left.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0028__true0__pred0__target0__3345_left.png (deflated 2%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0027__true0__pred0__target0__22425_right.png (deflated 1%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0011__true2__pred4__target4__20856_right.png (deflated 2%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0006__true0__pred0__target0__27541_right.png (deflated 10%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0016__true0__pred0__target0__580_right.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0026__true0__pred0__target0__17521_right.png (deflated 1%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0015__true4__pred3__target3__39495_right.png (deflated 1%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0024__true3__pred2__target2__39904_left.png (deflated 12%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0023__true0__pred0__target0__1700_left.png (deflated 4%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0014__true0__pred0__target0__30604_left.png (deflated 9%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0021__true0__pred0__target0__16879_left.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0008__true0__pred0__target0__31731_left.png (deflated 13%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0000__true0__pred0__target0__15072_right.png (deflated 2%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0025__true0__pred0__target0__11363_right.png (deflated 1%)\n  adding: kaggle/working/outputs/gradcam/gradcampp_val_pred/0012__true0__pred0__target0__11524_right.png (deflated 11%)\n  adding: kaggle/working/outputs/eval/ (stored 0%)\n  adding: kaggle/working/outputs/eval/metrics.json (deflated 34%)\n  adding: kaggle/working/outputs/eval/classification_report.txt (deflated 59%)\n  adding: kaggle/working/outputs/eval/predictions.csv (deflated 73%)\n  adding: kaggle/working/outputs/eval/confusion_matrix.png (deflated 17%)\n","output_type":"stream"}],"execution_count":40}]}